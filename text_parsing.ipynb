{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/filecommits.csv\"\n",
    "protocols = pd.read_csv(path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "protocols[\"ts\"] = protocols.DateCommit.apply(lambda x: pd.datetime.fromtimestamp(x).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Author</th>\n      <th>DateCommit</th>\n      <th>text</th>\n      <th>ts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>day18_20201021.md</td>\n      <td>jakoliendenhollander</td>\n      <td>1603351976</td>\n      <td>Day 18 - Wednesday 21 October 2020 Gradient De...</td>\n      <td>2020-10-22</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>day33_20201111.md</td>\n      <td>kevintomas1995</td>\n      <td>1605115641</td>\n      <td>Day 33 - Wednesday, 11th Novmeber 2020 Cluster...</td>\n      <td>2020-11-11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>day7_20201006.md</td>\n      <td>kevintomas1995</td>\n      <td>1601997748</td>\n      <td>Day 7 - Tuesday, 6 October 2020 Practicing dec...</td>\n      <td>2020-10-06</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>day21_20201026.md</td>\n      <td>kevintomas1995</td>\n      <td>1603787443</td>\n      <td>Day 21 - Monday, 26th October 2020 Naive Bayes...</td>\n      <td>2020-10-27</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>day14_20201015.md</td>\n      <td>PetraPi</td>\n      <td>1602781622</td>\n      <td>Day 14 - Thursday, 15th October 2020 Finalize ...</td>\n      <td>2020-10-15</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "            Filename                Author  DateCommit  \\\n0  day18_20201021.md  jakoliendenhollander  1603351976   \n1  day33_20201111.md        kevintomas1995  1605115641   \n2   day7_20201006.md        kevintomas1995  1601997748   \n3  day21_20201026.md        kevintomas1995  1603787443   \n4  day14_20201015.md               PetraPi  1602781622   \n\n                                                text          ts  \n0  Day 18 - Wednesday 21 October 2020 Gradient De...  2020-10-22  \n1  Day 33 - Wednesday, 11th Novmeber 2020 Cluster...  2020-11-11  \n2  Day 7 - Tuesday, 6 October 2020 Practicing dec...  2020-10-06  \n3  Day 21 - Monday, 26th October 2020 Naive Bayes...  2020-10-27  \n4  Day 14 - Thursday, 15th October 2020 Finalize ...  2020-10-15  "
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protocols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(s):\n",
    "    s = re.sub('[^\\s]*.com[^\\s]*', \"\", s)\n",
    "    s = re.sub('[^\\s]*www.[^\\s]*', \"\", s)\n",
    "    s = re.sub('[^\\s]*.co.uk[^\\s]*', \"\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols['CleanText'] = protocols['text'].map(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#text = \"This is my text. It icludes commas, question marks? and other stuff. Also U.S..\"\n",
    "#tokens = tokenizer.tokenize(text)\n",
    "\n",
    "protocols[\"tokens\"] = protocols[\"CleanText\"].map(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "fake = Faker()\n",
    "fake_names = []\n",
    "\n",
    "authors = protocols['Author'].unique()\n",
    "for i in range(len(authors)):\n",
    "    fn = {}\n",
    "    fn[\"author\"]=authors[i]\n",
    "    fn[\"fakename\"]=fake.name()\n",
    "    fake_names.append(fn)\n",
    "\n",
    "fn_df = pd.DataFrame()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}