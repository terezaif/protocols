{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/filecommits.csv\"\n",
    "protocols = pd.read_csv(path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the name of the person that is duplicate\n",
    "protocols['author'] = protocols['author'].replace(['William Franklin'],'Jennifer Montgomery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "author\nAmy Williams           3\nBonnie Brown           3\nBonnie Williams        3\nCharles Christensen    3\nErin Robinson          3\nJacob Phillips         3\nJennifer Montgomery    3\nLarry Sanders          3\nNicole Johnson         3\nThomas Hansen          3\nTimothy Stevens        3\nWilliam Rodriguez      3\nZachary Brooks         3\ndtype: int64"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ideally all have 3 protocols per person ( manual step done on the text file: swap author of day 2 w name from day 16)\n",
    "protocols.groupby('author').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "protocols[\"ts\"] = protocols.date_commit.apply(lambda x: pd.datetime.fromtimestamp(x).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(s):\n",
    "    s = re.sub('[^\\s]*.com[^\\s]*', \"\", s)\n",
    "    s = re.sub('[^\\s]*www.[^\\s]*', \"\", s)\n",
    "    s = re.sub('[^\\s]*.co.uk[^\\s]*', \"\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols['clean_text'] = protocols['text'].map(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#text = \"This is my text. It icludes commas, question marks? and other stuff. Also U.S..\"\n",
    "#tokens = tokenizer.tokenize(text)\n",
    "\n",
    "protocols[\"tokens\"] = protocols[\"CleanText\"].map(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenising with Spacy\n",
    "protocols['tokens_clean_text'] = protocols['clean_text'].map(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols['sentences_clean_text'] = protocols['tokens_clean_text'].apply(lambda toks: list(toks.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 Day  True False False Xxx False\n"
     ]
    }
   ],
   "source": [
    "for token in protocols['tokens_clean_text'][0]:\n",
    "    print (token, token.idx, token.text_with_ws,token.is_alpha, token.is_punct, token.is_space,token.shape_, token.is_stop)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#things we want to do:\n",
    "\n",
    "# get number of words that are not punctions and so on\n",
    "\n",
    "# get median word lenght\n",
    "# Entities and Sentiment? :)\n",
    "\n",
    "\n",
    "# get number of tokens per document\n",
    "protocols[\"token_count\"] = protocols['tokens_clean_text'].map(len)\n",
    "# get number of sentences\n",
    "protocols[\"sentence_count\"] = protocols['sentences_clean_text'].map(len)\n",
    "\n",
    "\n",
    "protocols[\"content_words_count\"] = protocols['tokens_clean_text'].apply(lambda toks: [token for token in toks if token.is_alpha is True and token.is_stop is False and token.is_punct is False])\n",
    "\n",
    "protocols[\"content_word_count\"] = protocols['content_words_count'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[Day,\n Wednesday,\n October,\n Gradient,\n Descent,\n Protocol,\n J,\n Schedule,\n Class,\n Gradient,\n Descent,\n Discussing,\n C,\n protocol,\n Day,\n Working,\n Notebooks,\n Gradient,\n Descent,\n Discussing,\n Notebooks,\n Working,\n Notebooks,\n Daily,\n standup,\n Working,\n Notebooks,\n Class,\n Gradient,\n Descent,\n Gradient,\n Descent,\n GD,\n explained,\n GD,\n optimize,\n functions,\n solving,\n equation,\n closed,\n form,\n functions,\n linear,\n regression,\n solving,\n equation,\n easy,\n add,\n features,\n solving,\n problem,\n takes,\n longer,\n GD,\n GD,\n find,\n theta,\n J,\n denoting,\n loss,\n function,\n minimum,\n Methods,\n parameters,\n included,\n greedy,\n GD,\n opposite,\n greedy,\n greedy,\n methods,\n method,\n scale,\n power,\n analogy,\n GD,\n finding,\n lowest,\n valley,\n map,\n Steps,\n GD,\n Define,\n function,\n area,\n area,\n denoted,\n theta,\n theta,\n formula,\n hypothesis,\n Define,\n loss,\n function,\n J,\n theta,\n theta,\n loss,\n function,\n uses,\n base,\n formula,\n calculating,\n mean,\n squared,\n errors,\n y,\n predicted,\n value,\n m,\n makes,\n rest,\n math,\n add,\n Set,\n starting,\n values,\n based,\n derivatives,\n area,\n function,\n Derivatives,\n theta,\n theta,\n defined,\n derivative,\n theta,\n number,\n theta,\n provides,\n find,\n minimum,\n slope,\n curve,\n consult,\n theta,\n theta,\n thetas,\n derivative,\n theta,\n needs,\n Note,\n starting,\n point,\n usually,\n Getting,\n direction,\n taking,\n small,\n steps,\n GD,\n takes,\n long,\n reach,\n minimum,\n taking,\n large,\n steps,\n GD,\n miss,\n minimum,\n Learning,\n rate,\n initial,\n alpha,\n Learning,\n rate,\n change,\n GD,\n Alpha,\n times,\n derivative,\n step,\n size,\n Step,\n size,\n gets,\n smaller,\n step,\n GD,\n getting,\n closer,\n minimum,\n Repeat,\n descent,\n improvement,\n achieved,\n GD,\n stops,\n loss,\n step,\n larger,\n loss,\n previous,\n step,\n non,\n convex,\n function,\n GD,\n stuck,\n local,\n mimimum,\n instead,\n ending,\n global,\n minimum,\n problem,\n occur,\n OLS,\n OLS,\n convex,\n function,\n minimum,\n Saddle,\n points,\n find,\n local,\n minimum,\n Local,\n minimum,\n find,\n saddle,\n points,\n local,\n minima,\n Requires,\n reading,\n Scaling,\n Scaling,\n makes,\n features,\n range,\n logarithm,\n housing,\n prices,\n GD,\n solve,\n equation,\n finds,\n minimum,\n important,\n features,\n scale,\n Scaling,\n limits,\n number,\n steps,\n required,\n GD,\n parameter,\n scaled,\n features,\n learning,\n rate,\n directions,\n parabola,\n OLS,\n solves,\n equation,\n features,\n require,\n scaling,\n Types,\n GD,\n Batch,\n GD,\n Uses,\n instances,\n dataset,\n Mini,\n batch,\n GD,\n Uses,\n sample,\n training,\n dataset,\n Stochastic,\n random,\n GD,\n Uses,\n random,\n instance,\n calculate,\n derivative,\n minimum,\n advisable,\n large,\n datasets,\n Note,\n time,\n Minimum,\n close,\n reached,\n GD,\n applied,\n entire,\n dataset,\n Reduces,\n learning,\n rate,\n find,\n minimum,\n Discussing,\n C,\n protocol,\n Tuesday,\n maximum,\n likelihood,\n loss,\n function,\n Maximum,\n likelyhood,\n type,\n loss,\n function,\n loss,\n function,\n defined,\n based,\n model,\n Maximum,\n likelihood,\n loss,\n function,\n logarithmic,\n regression,\n linear,\n regression,\n use,\n squares,\n loss,\n function,\n decision,\n boundary,\n threshold,\n Yes,\n set,\n boundary,\n run,\n model,\n adjust,\n threshold,\n need,\n form,\n decision,\n boundary,\n depends,\n formula,\n linear,\n regression,\n boundary,\n straight,\n line,\n polynomial,\n regression,\n boundary,\n curvy,\n Discussing,\n notebooks,\n afternoon,\n Changes,\n theta,\n step,\n Theta,\n optimized,\n step,\n generally,\n smaller,\n axis,\n theta,\n directions,\n Theta,\n updated,\n individually,\n axis,\n direction,\n change,\n theta,\n step,\n differs,\n axis,\n Theta,\n smaller,\n direction,\n direction,\n direction,\n change,\n depends,\n derivative,\n step,\n theta,\n gets,\n updated,\n step,\n times,\n derivative,\n Think,\n corner,\n rubix,\n cube,\n changes,\n planes,\n turned,\n Types,\n GD,\n main,\n difference,\n types,\n GD,\n observations,\n included,\n step,\n GD,\n Batch,\n GD,\n uses,\n observations,\n step,\n normal,\n GD,\n relatively,\n small,\n datasets,\n Mini,\n Batch,\n GD,\n uses,\n n,\n observations,\n step,\n step,\n performed,\n n,\n observations,\n number,\n observations,\n step,\n stochastic,\n GD,\n Stochastic,\n GD,\n uses,\n random,\n observation,\n step,\n step,\n performed,\n random,\n observation,\n features,\n observations,\n dataset,\n Mini,\n Batch,\n GD,\n Stochastic,\n GD,\n accuracy,\n lowered,\n optimization,\n Mini,\n Batch,\n GD,\n Stochastic,\n GD,\n large,\n datasets,\n topics,\n starting,\n point,\n independent,\n algorithm,\n cost,\n functions,\n second,\n degree,\n functions,\n parabola,\n Theta,\n denotes,\n parameters,\n weights,\n Theta,\n notebooks,\n theta,\n referred,\n class,\n create,\n values,\n particular,\n range,\n suitable]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = protocols['tokens_clean_text'][0]\n",
    "\n",
    "only_toks = [token for token in tokens if token.is_alpha is True and token.is_stop is False and token.is_punct is False]\n",
    "only_toks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}