,Filename,Author,DateCommit,text
0,day18_20201021.md,Kristina Lin,1603351976,"Day 18 - Wednesday 21 October 2020 Gradient Descent  Protocol by Jakolien Schedule  09:00 Class on Gradient Descent 10:00 Discussing Chandra's protocol about Day 17 11:00 Working on Notebooks on Gradient Descent 13:00 Discussing Notebooks 14:00 Working on Notebooks 16:00 Daily standup 16:45 Working on Notebooks  Class on Gradient Descent Gradient Descent (GD) explained GD is used to optimize functions without solving the equation. In closed form functions, such as linear regression, solving the equation is easy. However, if you add more features, solving the problem takes longer. Hence, GD is used. GD is used to find theta for which J, denoting the loss function, is at its minimum. Methods in which all parameters are included are greedy. GD is the opposite: not greedy. Not greedy methods are commonly used, because the method has to scale with the available computation power. An analogy to GD is finding the lowest valley on a map. Steps in GD 1. Define function or area. * The area is denoted as theta 0 and theta 1 in the formula of the hypothesis h. 2. Define the loss function J for theta 0 and theta 1. * The loss function uses the same base formula as for calculating the mean of squared errors. * y is the predicted value. * 1 over 2m makes the rest of the math add up. 3. Set starting values based on derivatives of your area function. * Derivatives for theta 0 and theta 1 are defined. * The derivative of theta 0 is just a number, while theta 1 provides an X. * To find the minimum or slope of a curve, consult theta 0 and theta 1. * If there are more thetas, the derivative of each theta needs to be computed. * Note: The starting point is usually 1 or 0. 4. Getting the direction. * ! When taking small steps, GD takes long to reach the minimum. * ! When taking large steps, GD may miss the minimum. * Learning rate is the initial alpha.   * Learning rate does not change throughout the GD. * Alpha times derivative is the step size.   * Step size gets smaller with every step when GD is getting closer to the minimum. 5. Repeat descent until no further improvement is achieved. * GD stops when the loss in the next step is larger than the loss in the previous step. * In a non-convex function, GD could get stuck in a local mimimum instead of ending up in the global minimum. * This problem does not occur in OLS, because OLS is a convex function, so it only has 1 minimum. Saddle points * Used to find a local minimum. * Local minimum can be used to find the next saddle points/local minima. * Requires further reading. Scaling * Scaling makes all features on a comparable range, e.g. between 0 and 1,   * e.g. the logarithm of the housing prices. * GD does not solve the equation, it only finds the minimum, so it is important for all features to have the same scale. * Scaling limits the number of steps required for GD of a parameter. * Using scaled features, the learning rate will be the same from both directions of the parabola. * OLS solves the equation, so features do not require scaling. Types of GD Batch GD * Uses all instances of the dataset. Mini batch GD * Uses a sample of the training dataset. Stochastic (random) GD * Uses one random instance to calculate derivative and minimum. * Only advisable in very large datasets. Note: time it. * Minimum will be close but will never be reached, because GD is not applied to the entire dataset. * Reduces the learning rate to find the minimum. Discussing Chandra's protocol of Tuesday  Is maximum likelihood the same as the loss function? Maximum likelyhood is a type of loss function. The loss function is defined based on your model. Maximum likelihood is used as a loss function for logarithmic regression. In linear regression, you use least squares as a loss function.   Is decision boundary the same as the threshold? Yes, it is. First set the boundary, then run the model and then adjust the threshold if need be. The form of the decision boundary depends on the formula. For linear regression, the boundary is a straight line. For polynomial regression, the boundary is curvy.    Discussing notebooks in the afternoon Changes of theta per step  Theta does not get optimized with every step, it does not generally get smaller. On one axis, theta can go in two directions. Theta is updated individually on each axis. The direction of change for theta with every step differs per axis. Theta may get smaller in one direction, but not in the other direction. The direction of change depends on the derivative. With each step, each theta gets updated by the step times the derivative. Think of the corner of a rubix cube, which changes all the planes when it is turned.  Types of GD  The main difference between the types of GD is how many observations are included in each step of the GD. Batch GD uses all observations in each step. This is the 'normal' GD. Used for relatively small datasets Mini Batch GD uses n observations in each step. The next step will be performed with another n observations. If the number of observations in each step is 1, it is stochastic GD (?). Stochastic GD uses one random observation in each step. The next step will be performed with another random observation. Used when you have more features than observations in the dataset. In Mini Batch GD and Stochastic GD the accuracy is lowered by reducing complexity (optimization). Mini Batch GD and Stochastic GD are used for large datasets.  Other topics * The starting point is independent of the algorithm. * All cost functions are second degree functions, so there is always a parabola. * Theta denotes parameters and weights.   * Theta in the notebooks is not the same theta as the one referred to during class. * Numpy.linspace can be used to create values in a particular range suitable for X."
1,day33_20201111.md,Mrs. Kelly Hale DVM,1605115641,"Day 33 - Wednesday, 11th Novmeber 2020 Clustering Protocol by Kevin ""God created war to teach Americans geography""  Schedule  9:00 - 9:45 Daily review 9:45 - 11:00 Individual time 11:00 - 12:00 Presentation on clustering 13:00 - 16:00 Working on notebooks 16:00 - 16:30 Daily Stand up  Clustering What is it about?  Clustering is about dividing a dataset into groups (cluster) These groups (clusters) can be then used for visualisations, analysing these groups in more depth or use them as features or outcomes for models Applications:  Anomaly detection Document clustering Recommendation engines Three groups of clustering methods: K-Means clustering Hierarchical clustering DBSCORE  K-means clustering  How does it work? Divides the data into K clusters Each described by the mean of the samples in the cluster. The means are commonly called the cluster “centroids” (does not refer to a single number, but to the vector of means of the variables) The K-means algorithm aims to choose centroids that minimise the within-cluster sum of squares This is ensured in iterating over two steps: Data assigment step: Each centroid defines one of the clusters. In this step, each data point is assigned to its nearest centroid Centroid update step: In this step, the centroids are recomputed. This is done by taking the mean of all data points assigned to that centroid's cluster The algorithm will then stop, if a stopping criteria is met   Great visualisation of how the algorithm works: https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ How to find the best number of clusters? Elbow-Method V-measure: best value is 1 Silhouett coefficient: best value is 1, worst value is -1, values around 0 indicates overlapping clusters Davis-Bouldin score: minimum score is 0 with lower values indicating better clustering Metrics: Homogeneity Comparable to precision   Completeness Comparable to recall   Advantages: simple algorithm ""fast food"" of clustering  scales well with large datasets   Disadvantages:  K-Means relies on random initialization     → Outcome depends on a random seed (you can also set the centroids)     → Will not find always same clusters Only identifies clusters with simple structures     Hierarchical clustering  Difference to k-means: Finding clusters does not depend on random initialization Two ways: Divisive method Assign all of the observations to a single cluster Then partition the cluster to two least similar clusters Finally, we proceed recursively on each cluster until there is one cluster for each observation   Agglomerative method Assign each observation to its own cluster Then, compute the similarity (e.g., distance) between each of the clusters and join the two most similar clusters Finally, repeat steps 2 and 3 until there is only a single cluster left   Three ways of comupting the distance between clusters: Single Linkage  Complete Linkage   Average Linkage    How to find the best number of clusters?  Plot a Dendrogram Decide on largest vertical distance with same # of clusters    Advantages: Easier to decide on the number of clusters by looking at the dendrogram  Easy to implement   Disadvantages:  Time complexity: not suitable for large datasets Very sensitive to outliers  DBSCAN  Views clusters as areas of high density separated by areas of low density Identifies points that don’t belong to any cluster Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped Centrally: concept of core samples, which are samples that are in areas of high density A cluster is therefore a set of core samples, each close to each other and a set of non-core samples that are close to a core sample (but are not themselves core samples)    Advantages: Identifies automatically # of clusters Able to identify complex clusters Able to identify clusters with very different sizes  Distinguishes between clusters (dense data) and noise → good for outlier detection   Disadvantages:     Great at separating high density clusters from low density clusters, but struggles with clusters of similar density Struggles with high dimensionality data  Links Capstone teams and ideas Notebooks Repo: https://github.com/neuefische/2020-ds-Dimensionality-Reduction"
2,day7_20201006.md,Mrs. Kelly Hale DVM,1601997748,"Day 7 - Tuesday, 6 October 2020 Practicing decent coding, pseudocode and VSCode Protocol by Kevin Schedule  9- 10: daily review of day 6 Going through yesterdays protocol on Introduction to decent coding & pseudocode, IDE & VSCode, TicTacToe coding practice Discussing Andreas' and Andrew's solutions of their implementation of the tictactoe game 10 - 12: class Tereza demonstrated her soultion of the tictactoe game Talking about blockers regarding the tictactoe assignment Further explanation of VSCode Introduction to the daily coding exercise and to pytest Setting up the virtual environment for the coding exercise 12 - 13: lunch break 13 - 13:30: group work Planning the coding exercise using pseudocode Presenting the pseudocode 14 - 16: group work Exercise: pair programming with VSCode  Exercise: Setting up the virtual environment for the coding exercise  Fork and clone the 2020-ds-Coding-2 repo from github using the terminal Open a terminal in VScode and create a new conda environment conda create --name reversecalculator python=3.8.5 Install pytest conda install -n reversecalculator pytest==6.1.1 Pytest is a framework which makes it easy to implement small tests Documentary: https://docs.pytest.org/en/stable/  Exercise: Pair programming with VSCode and solve the reverse polish calculator problem  First activate the necessary virtual environment called reversecalculator with conda activate reversecalculator Code the rpn_calculator explanation of the reverse polish calculator in general: https://github.com/neuefische/2020-ds-Coding-2/blob/main/Reverse-Polish-Calculator.md Minimal coding practices to keep in mind: Functions should have explicit names Functions should have docstrings explaining how to call them Functions should be between 4-10 lines of code long "
3,day21_20201026.md,Mrs. Kelly Hale DVM,1603787443,"Day 21 - Monday, 26th October 2020 Naive Bayes Algorithm, Scaling and Hyperparameter Tuning Protocol by Kevin Schedule  9:00 - 9:30 Daily review 9:30 - 11:30 Groupwork on Naive Bayes Algorithm 11:30 - 12:30 Presentations 13:30 - 16:00 Working on notebooks 16:00 - 16:30 Daily Stand up   Groupwork on Naive Bayes Algorithm What is it about?  Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem Works on conditional probability that something will happen, given that something else has already occurred    Three ways of Naive Bayes Gaussian: Can be applied to any continuous data Bernoulli: Can be applied to binary data, mostly used for text data classification Multinomial: Can be applied to count data, mostly used for text data classification  How does the algorithm work?  Following steps can be identified: Convert data into a frequency table Create likelihood table by calculating the probabilities of each weather condition and playing Use Naive Bayesian equation to calculate the posterior probability for each class The class with the highest posterior probability is the outcome of prediction  Implementation in Python  Import it from the sklearn package Use it like every other model in sklearn model = GaussianNB() model.fit(...) model.predict(...) https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussiannb#sklearn.naive_bayes.GaussianNB  Limitations  Conditional Independence All events of the data set are independent of one another Each event contributes equally to classifying the outcome Generally speaking: Very strong assumptions are made, which will not always apply to real life  Advantages  Quick to train and predict outcome of new data points (faster than linear models) Can handle missing feature values well Can make probabilistic predictions Can handle continuous and discrete data  Requires less RAM  Disadvantages  Often slightly worse in terms of generalization If a certain category was not observed in training set, the model will assign a probability of 0 Assumes independent predictors which is almost impossible in real life  Applications of the algorithm  Most common: text data classification Real-time prediction Multi-class prediction Recommendation systems  Scaling Why do we need scaling?  Many times, the input features of your model have different units Therefore the variables have different scales Tree-based models like decision tree or random forest are unaffected by the scale of numerical input variables But, many machine learning algorithms using distance measures (e.g. KNN, SVM) perform better when the input features are scaled to a specific range Most popular ways: normalization and standardization  Normalization  All values will lie within the range of 0 and 1 (at least in the default mode) Following normalization is done: y = (x – min) / (max – min) The implementation in python is done again with sklearn: scaler = MinMaxScaler() scaler.fit_tranform(X_train) scaler.tranform(X_test) https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html  Standardization  Standardization means, that the data is rescaled in a way that the mean of observed values is 0 and the standard deviation is 1 Simply said: the data is ""centered"" Following normalization is done: y = (x – mean) / standard_deviation with mean = sum(x) / count(x) and standard_deviation = sqrt( sum( (x – mean)^2 ) / count(x)) The implementation in python is done again with sklearn: scaler = StandardScaler() scaler.fit_tranform(X_train) scaler.tranform(X_test) https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html  Cross validation (cv)  Occurs both in scaling and hyperparameter tuning It is also called k-Fold Cross Validation, where k stands for the number of groups that a given data sample is to be split into Generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split Avoids overfitting the model   Hyperparameter Tuning What is a hyperparameter and why do we need to ""tune"" them?  Hyperparameters are parameters, which cannot be estimated by the model from the given data These parameters are used to estimate the model parameters Hyperparameter tuning is the process of determining the right combination of hyperparameters that allows the model to maximize model performance Setting the correct combination of hyperparameters is the only way to extract the maximum performance out of models. Two ways of hyperparameter tuning in python: GridSearchCV and RandomizedSearchCV both implemented with sklearn  GridSearchCV  Performs an exhaustive search over a prior defined parameter space using cross-validation It will evaluate all of the possible parameter combinations of the search space in order to find and return the best combination That's why it can be very time-comsuming https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html  RandomizedSearchCV  Random search will not try every possible combination of our search space but will randomly pick and evaluate parameter combinations Better performance, but it is likely that the GridSearchCV will find a better solution https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html  Notebooks Repo: https://github.com/neuefische/2020-ds-Diabetes-Challenge 1. Scaling_HyperparameterTuning 2. Diabetes Challenge 3. Diabetes Challenge - Solution Presentations https://github.com/neuefische/hh-2020-ds4-daily-review/tree/master/naive_bayes_algorithm"
4,day14_20201015.md,Kelsey Joseph,1602781622,"Day 14 - Thursday, 15th October 2020 Finalize the EDA project and presentations Protocol by Petra Schedule   9.00 - 9.15: Daily review of day 13    09.15 - 12.00: Finalizing the Notebooks  Finishing notebooks Uploading notebooks and python scripts to our GitHub Account If enough time, thinking about how to present    13.00 - 17.00: Presentations of EDA Projects    Reminder of our stakeholders:   Who | Stakeholder | Characteristics -----|-------|------ Ole | Buyer | 5 kids, no money, nice (social) neighbourhood, Timing?, Location? Petra | Seller | Invest with big returns, renovation?, which Neighbourhood? Timing? Andrew | Seller | Has house moves soon (timing?), high profit in middle class NH Jakolien | Buyer | Waterfront , limited budget, nice & isolated but central neighbourhood without kids (but got some on her own) Michael | Buyer | Lively, central neighbourhood, middle price range, right timing (within a year) Sina | Buyer | High budget, wants to show off, timing within a month, waterfront, renovated, high grades, resell within 1 year Martina | Seller | Has several houses, some in bad neighbourhoods, willing to evict people, timing?, big returns, open for renovations Kevin | Buyer | 2 people, country (best timing & unrenovated) & city house (fast & central location) Andreas | Buyer | Invest in poor neighbourhood, buying & selling, costs back + little profit, socially responsible Sebastian | Buyer | Unlimited Budget, 4+ bathrooms or smaller house nearby, big lot (tennis court & pool), golf, historic, no waterfront Jennifer | Seller | Invests in historical houses, best NHs, high profits, best timing within a year Chandra | Seller | Owns expensive houses, needs to get rid, best timing within a year, open for renovation when profits rise Laura | Seller | Italian mafiosi, sells several central houses(top10%) over time, needs average outskirt houses over time General feedback/advice based on our presentations: EDA:  Import all packages at the beginning Structure Use checklist for structure   Give an overview over task Make clear assumptions Clear code with explanations Transform data for better visualization Find additional data (e.g. bars in certain areas) Always clean your notebooks  Training and tesing the model (supervised machine learning):  Use ""raw"" data Feature engineering:  creating new explanatory variables to be used in your model   Feature selection: selecting the fetaures(predictors) for your model   Everything you do (removing outlier), only do it on your train dataset Never remove oulier based on target value Watch out not to use the target value as feature Do not use transformed values or keep in mind that you have to retransorm after analysis For a guideline on how to evaluate your model: https://en.wikipedia.org/wiki/Gauss–Markov_theorem    Presentation:  General: Explain your thought process  Bring things into perspective, for example use persentages instead of total numbers Give specific recommendations/ have a clear message in the end Present main finding at the beginning and then explain how you came to that conclusion Visualizations Use different visualization of content to show different perspectives  Avoid redundant plots or information If you have multiple groups, plot them next to each other Use the same colour (palette) for your plots to make it coherent   Give an outlook/future work  Watch your time   When talking to stakeholders:  Good ""Story Telling"" Use non-technical ""easy"" language Self-explaing graphs and less text only talk about the finding that is interesting/ relevant for the stakeholder  If you want to look at all the presentations you can find them on our profiles."
5,day10_20201009.md,Sara Dawson,1602449789,"Day 10 - Friday, 9th October 2020 More Pandas running around and introduction to Numpy Protocol by Michael Today's Schedule  09:00 - 09:45: Daily review of day 10 pd.crosstab() within Pandas library: The pandas crosstab function builds a cross-tabulation table that can show the frequency with which certain groups of data appear. https://pbpython.com/pandas-crosstab.html   pd.pivot_table() within Pandas library: A pivot table is a table of statistics that summarizes the data of a more extensive table (such as from a database, spreadsheet, or business intelligence program). This summary might include sums, averages, or other statistics, which the pivot table groups together in a meaningful way.  https://en.wikipedia.org/wiki/Pivot_table https://pbpython.com/pandas-pivot-table-explained.html    pd.crosstab()vs. pd.pivot_table()  https://stackoverflow.com/questions/36267745/how-is-a-pandas-crosstab-different-from-a-pandas-pivot-table#:~:text=3%20Answers&text=The%20main%20difference%20between%20the,the%20column%20names%20as%20strings    09:45 - 10:45: Answering questions & Introduction to the day's activities  Assignment after daily: finishing 5-Pandas_Exercise3 Access different Jupyter Notebooks at the same time:open a higher level directory ($ cd ) Options for plotting within python seaborn matplotlib   Usage of pd.iloc[]: Returns slices of the data set (e.g TOP 3, TOP 10 etc.) Creating a dummy = Binary variable conversion Docstrings have to be implemented right after the function definition with ``` $ python --version: To find out the version of python GitHub: There are different rights for merging data (read, write)  .gitignore-File: Ignores files while pushing stuff to GitHub Include file names to be ignored in the .gitignore-file $ git status: Files which showed up before are gone after including them in the .gitignore-file $ git add .: Add all changes Adding files to a pull-request $ git pull $ git add .   10:45 - 12:00: Exercise Jupyter-Notebook: 5-Pandas_Exercise3 12:00 - 13:00: Lunch break 13:00 - 13:30: Check in with Mia and Tereza 13:30 - 15:30: Introduction into Exploratory Data Analysis & Start of group assignment 16:00 - 16:30: Daily check out 15:30 - 17:30: Group assignment   THE END  Repo - 2020-ds-Pandas-Numpy Notebook 5: Pandas Exercise 3 file: 5-Pandas_Exercise3  Content: Contains tasks regarding Pandas functions(especially pd.cut() and pd.pivot_table())  Introduction into Exploratory Data Analysis (EDA) file: 5_EDA.pdf  EDA can answer business related questions Detective like work mode Confirm the expected & show the unexpected Notes regarding .pdf-file Two things to look at a distribution: Appearance of/ Separation into groups  Skew of a distribution (https://www.expii.com/t/normal-distribution-right-and-left-skewed-graphs-5338)      When presenting plots Graph should be self explanatory Noticing important content should be easy (see what should be seen) Data can be summarized in two ways By location - mean, median, mode By variability - variance, standard deviation, range Binary vs. categorical data Binary, e.g. boolean values like True and False Pie and bar charts mainly used for categorical data Numerical data Estimation of location Mean - sensitive to outliers Trimmed-mean - not sensitive to outliers (trimmed-mean = mean after removing extreme values) Median - not sensitive to outliers (50th percentile - splits data in the middle)   Estimation of variability Variance - mean squared error Standard deviation MAD != Mean absolute deviation MAD = Median absolute deviation   Estimation of variability contd Range - max_value - min_value Ranks - rank of sorted values (sorted statistics - from min to max or from max to min) percentile (quantile) Interquartile range - P75 - P25   Visualizing data distribution Not more then 2-3 overlapping graphs   Visual data summaries, e.g. box plots Box plots   Visualizing relations between variables/values, e.g. scatter plots Correlation between data can be seen Pearson coefficient 0 - No correlation 1, -1 - Highest correlation 1 >= r >= 0 - The higher the more (graph raises from bottom left to top right) -1 <= r <= 0 - The higher the less (graph drops from top left to bottom right)   Spearman coefficient p rank based nonparametric measure of rank correlations For small data sets Robust to outliers   Correlation Correlation != causality - Beware of fake correlations   kaggle assignment kaggle is an online community for data scientist. The main purpose of kaggle is the organization of data science competitions. 20-min.-kaggle-Assignment: Find two notebooks. One that is in your interpretation good and one that is really bad.   Group assignment Assignment - https://docs.google.com/document/d/1cX9oiTXJ8wdcTwuz7aUqmuJ-N-KMYnQ95Jcf0FY2PmA/edit Preparation today, presentation on monday.  20-min.-kaggle-Assignment Team-Twitter 'good'- Examples  EDA and Preprocessing for BERT (https://www.kaggle.com/parulpandey/eda-and-preprocessing-for-bert#2.-General-EDA)  'bad'-Examples  Tweet sentiment - Insight EDA (https://www.kaggle.com/raenish/tweet-sentiment-insight-eda)  Team-Walmart 'good'-Examples  Back to (predict) the future (https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda) M5 Forecasting - Starter Data Exploration (https://www.kaggle.com/robikscube/m5-forecasting-starter-data-exploration)  'bad'-Examples  Time series - Forecast - EDA, FE & Med (https://www.kaggle.com/anshuls235/time-series-forecasting-eda-fe-modelling) M5 comp: EDA + Models (https://www.kaggle.com/tarunpaparaju/m5-competition-eda-models)  Thinks we pointed out presenting the notebooks Dos | Don'ts  -----|------- Confirm the expected vs. show the unexpected | Title | Misleading title Table of content | UnreadableMissing legend Axis title | Going against common sense (red vs. green highlighting) Make groups comparable(Comparable graphs [axis, overlap etc.]) | Scales not comparable Consistency | Confusing Data overview | Too much details Easy to follow | Redundant content Get message directly | Not self explanatory Interactive plots | No descriptions Heat map | Extreme, bright colors Group assignment Check out the main visualization modules available in Python, namely matplotlib, seaborn, bokeh and plotly.  Based on the online documentation and example libraries, prepare a 10 minute presentation covering the following topics: * Functionalities: Using a very basic example to describe the major functionalities of the respective module. * Set-up: What has to be observed setting up the module? * Pros-Cons: What are the major advantages and disadvantages?"
6,day25_30102020.md,Kelsey Joseph,1604241837,"Day 25 - Friday, 30 October 2020  Protocol by Petra Schedule   9:00 - 10:00: Daily review of day 24   10:00 - 11:00: Start of second project   11:00 - 12:00: Team work second projekt    1200 - 13:00: Lunch break   13:00 - 16:00: Team work second projekt    16:00 - 16:30: Q & A   17:00 - open end: Helloween Zoom Party   Daily review: questions/ discussions  How is the feature imporatance calculated in Adaboost? number of features used in the stumps based on the squared error    When do we make feature selection? What is the best work flow? First feature importance and then model optimization with grid search    The Fox gives advice  Make a repo on Git Hub that you use as a team  Look at ""Git Hub baby steps"" shared by Tereza on Slack on 29 October  Make the repo public, so that you can work in parallel and use code reviewer Review NB   Set up a Conda environment  Make a very brief EDA to check what data you have Define the business case Good excercise for job applications, before job interview always ask yourself ""What problems could I solve in the company?""    Push your changes regularly/ daily to Git Hub Make in your Commits explicit what changes you intended The reviewer can then check if the code is really doing what it is supposed to do Keep in mind: ""You can always be hit by a bus!!""   Do not push Data to GitHub, its too big  Q & A  General remarks on first steps: Andreas and Chandra have a messy data set Kickstarter dataset has to be combined Unknown Columns in datasets    Helloween"
7,day6_20201005.md,Scott Small,1601928691,"Day 6 - Monday, 5 October 2020 Introduction to decent coding & pseudocode, IDE & VSCode, TicTacToe coding practice Protocol by Jenny Schedule  9- 10: daily review of day 5 going through yesterdays protocol on Unix Tutorial & Introduction to AI discussing and answering open questions on ""Pull"" command in github Overview slide by Andrew: https://docs.google.com/presentation/d/1QGW5ex1InZYMWd7UGoPAo-sW0cke7zNVJ3Qw-xSyRrM/edit?usp=sharing Further info: https://guides.github.com/activities/forking/ & https://dev.to/mollynem/git-github--workflow-fundamentals-5496 10 - 12: class Introduction to decent coding Introduction to IDE and VSCode Exercise: Run the python program in VSCode Exercise: Pair programming with Github and VSCode 12 - 13: lunch break 13 - 17: class Planning an algorithm using pseudocode Exercise: TicTacToe coding practice 17: end of class Daily standup: What have I learned today? What will we do tomororow? Are there any blockers?  Introduction to decent coding  Machine Learning in Production (Circle of Training & Testing a model): https://medium.com/microsoftazure/9-advanced-tips-for-production-machine-learning-6bbdebf49a6f Notebooks vs scripts:   Notebooks (e.g. Jupyter) | scripts (e.g. VSCode) ----------|-------- Exploratory data analysis (EDA) | EDA Feature engineering | Feature engineering Data cleaning |  Data cleaning Modeling | Modeling n.a. | Evaluation n.a. | Monitoring Introduction to IDE and VSCode  IDE (Integrated Development Environment) is a software application that provides comprehensive facilities to computer programmers for software development (https://en.wikipedia.org/wiki/Integrated_development_environment) VSCode (Visual Studio Code) is a free source-code editor made by Microsoft. Features include live pair programming and useful extensions. Download VSCode here: https://visualstudio.microsoft.com/services/live-share/  Exercise: Run the python program in VSCode  Clone the daily review repo from github using the terminal: git clone https://github.com/neuefische/hh-2020-ds4-daily-review Open a terminal in VScode and run the python script: python pairing/random_groups.py Export output to a file: python pairing/random_groups.py > random_pairing.txt Ways to get the file to be in the pairing folder:  mv random_pairing.txt pairing_folder use drag n'drop to move the file python random_groups.py > pairing_folder/random_pairing.txt  Exercise: Pair programming with Github and VSCode  Go online and set up Visual Studio Live Share in VSCode: https://visualstudio.microsoft.com/services/live-share/ Copy the link and invitite someone to pair programming in VSCode via Slack Check out what happens if you type in ipython: opens help menue  Planning an algorithm using Pseudocode  There is no strict set of standard notations for pseudocode, but some of the most widely recognised are: INPUT – indicates a user will be inputting something OUTPUT – indicates that an output will appear on the screen WHILE – a loop (iteration that has a condition at the beginning) FOR – a counting loop (iteration) REPEAT – UNTIL – a loop (iteration) that has a condition at the end IF – THEN – ELSE – a decision (selection) in which a choice is made  any instructions that occur inside a selection or iteration are usually indented   Example  REPEAT OUTPUT 'What is the best subject you take?' INPUT user inputs the best subject they take STORE the user's input in the answer variable IF answer = 'Computer Science' THEN OUTPUT 'Of course it is!' ELSE OUTPUT 'Try again!'  UNTIL answer = 'Computer Science'   Further info on pseudocode: https://blog.usejournal.com/how-to-write-pseudocode-a-beginners-guide-29956242698   Exercise: TicTacToe coding practice  Task 1: Create a new Conda environment called 'tictactoe' conda create --name tictactoe python=3.8.5 conda install -n tictactoe pip Task 2: Solve the TicTacToe assignment  Clone the repo 2020-DS-CODING to your mashine Activate environment in VSCode before starting to code using conda activate tictactoe Do the psuedocode Code TicTacToe game "
8,day29_20201105.md,Joy Williams,1604606313,"Day 29 - Thursday, 05 November 2020 R2 or not R2  Protocol by Andrew Schedule  9:00 -  9:15 Daily review of day 28 09:15 - 11:00 Finalise presentations and try to figure out when the presentations will start 11:00 - 13:00 Presentations 11:00 - 11:20 Kevin / Sina: Employee Drug Testing 11:20 - 11:40 Andrew / Jenny: Kickstarter Success!!! 11:40 - 12:00 Andreas / Chandra: Homeloan defaults 12:00 - 12:20 Laura / Sebastian: Darknet Drugs 12:20 - 12:40 Martina / Ole / Petra: Kickstarting Cat Memes 12:40 - 13:00 Jakolien / Michael: Kickstarter Profit Maximization 14:00 - 14:XX Github Refresher 15:00 - 16:00 Last minute fights with computer to get scripts running  Git help  Use branches for specific topics to keep work separete. Two people should not work in the same Jupyter Notebook on different branches. Creating new branches:    To create a branch in the terminal use: git branch <branch_name>   to switch into a branch use:   git swtich <branch_name>   to sync the newly created branch and push:   git push --set-upstream origin <branch_name>   Python Scripts  Use functions to keep your code simple. Once you have a piece of code that you like, for example building a chart or doing data cleaning, put this into a script library file. You can import functions from a .py script as you would other libraries. from <python_script> import <function> When calling functions use the path to find them, in the path '/' is replaced by '.', Numbers in directory names in the path cause problems and need to be avoided.  Feedback on Presentations General   Slides are free, don't be afraid to split content across multiple slides rather than cramming onto 1.   Reduce text on slides in favour of pictures, icons and symbols   Charts   Keep charts as simple as possible.   Correlation matrix should be 'masked' to reduce redundancy.   Correllation matrix is overly complex model to show lay people.   Box Plots are complex and many people are not familar with how to read them. Try something else, like a histogram.   Charts in presentations should have readable text labels and axes.   Chart colouring should be 'colour blind aware'. This means do not use Red and Green. Orange and Blue is better.   If information can be show in a simpler way then do that.   Scores and Statistical Jargon:   Always keep the stakeholder in mind when presenting information and how they will understand what is shown.   Less focus on statistical scores and more insight into what they tell us about the possible risks and rewards so that the stakeholder can use the information to make a decision.   Use MAPE as a model scorer for continuous target variables that are have a large range. Uses percentage error rtather than absolute. 'It is ok for large values to have large errors as long as small values have small errors'   It is easier to explain R2D2 than it is to explain R2. Don't talk about R2 in non-technical presentations unless you are the first person to be able to describe it in one sentence.  "
9,day3-20200930.md,Joy Williams,1601536681,"Day 3 - Wednesday, 30 September 2020 Introduction to the Basics of Programming in Python Protocol by Andrew Today's Schedule  9 - 10: daily review of day 2 going through the protocol related to setting up machine and tools discussing and answering open questions How do pull requests work? What is the difference between Rapid Assesment Test (RAT) and End of Day (EOD) questions?   10 - 10:30: Introduction to the day's activitiess Task for the day: Work on the codealong notesbooks in the respository that was cloned the day before plus additional repo. 1st Repo: https://github.com/neuefische/2020-ds-Intro-to-DS-toolbox 2nd Repo: https://github.com/neuefische/2020-ds-Python1   10:30 - 12: Work through notebooks Update progress in the google sheet: https://docs.google.com/spreadsheets/d/1wk4CyQJixCC2y3LVHSeMvMaixjeVeccHnWaRQOZ7HjQ/edit?usp=sharing Use slack channel #ask-mia-and-tereza if stuck 12 - 13: lunch break 13 - 13:30: Check in with Mia and Tereza 13:30 - 16: Continue pair programming to work through the assigned tasks 16 - 16:30: Answer any outstanding questions and organisational topics Daily standup: What hav I learned today? What will we do tomororow? Are there any blockers? No End of day questions (EOD) end of class  Today's Topics First Repo: Intro to Jupyter Notebook, Python and Bash repo: https://github.com/neuefische/2020-ds-Intro-to-DS-toolbox Notebook 1 Topics: Basics of Programming in Python file: 1-first-codealong.ipynb  The codealong notebooks provide insight into the following topics of getting started with Jupyter and Python Working with Jupyter  Jupyter Notebook and Running Cells Importing Packages Jupyter Notebook Cell Types Running Bash Commands  Basic Python  Loading a DataFrame Python Comments Accessing Methods from Packages and Objects Pulling up Docstrings This is very useful and saves a lot of googeling  You can do this by writing '?' after the method and running the cell or by pressing shift+tab within the parentheses of a method Variables Built-in Python Functions Common DataFrame Methods Pandas Series List and Series Slices Common Series Methods Graphing  Notebook 2 Topics: Coding best Practices and good habits file: 2-coding-best-practices-PEP8.ipynb   This codealong introduces a standard style guide for coding in python PEP8  Working with Whitespace Line Length Variable Names Indentation (tabs vs spaces fight to the death) get more info here: https://www.python.org/dev/peps/pep-0008/  Notebook 3 Topics: Most import and basic Bash commands file: 3-bash-shell.ipynb  Very quick review of navigating in the terminal The Bash Shell  pwd cd ls Tab completion  Second Repo: Python - Basics: Repo: https://github.com/neuefische/2020-ds-Python1 Notebook 1 Topics: More Basics of Programming in Python repo: 2020-ds-Python1  file: 1-Variables.ipynb The codealong notebooks provide insight into the following topics of getting started coding in Python Introduction to Variable Types and Numeric Operators  Numeric Variable Types Numeric Operations Variables  Notebook 2 Topics: Introduction to if-Statements file: 2-if-Statement.ipynb This codealong introduces how to work with logic in Python If Statements  Logic Conditionals Using the 'If' Elif and Else And, Or and Not  Notebook 3 Topics: Looping file: 3-while-Loop.ipynb This codealong introduces how to use iteration While Loops  While Flow Infinite Loops (Unlike infinite fruit loops, they are not good) Control Flow (Continue, Break and Pass)  Notebook 4 Topics: Introduction to Strings file: 4-Strings.ipynb This codealong introduces just a few of the huge array of string operations Strings  String Operations Working with characters in Strings The Awesome art of Slicing Iteration and Strings For Loops (aka forget about using 'while' loops) Formatting This is worth practicing "
10,day40_20201120.md,Cole Schultz,1605969360,"Day 40 - Friday, 20 November 2020 Review on RNN & Error Analyis Protocol by Sina  Schedule   09:00 - 09:30: Daily review of day 39   09:30 - 11:00: Work on RNN Notebooks   11:00 - 12:30: Q&A and Error Analysis    12:30 - 13:30: Lunch break   13:30 - 16:15: Work on Kaggle Notebook on Error Analysis   16:15 - 17:45: Review on Error Analysis & Check-Out   Q & A Session with Mia and Tereza concerning RNN Notebook 2:  Model predicts continuous value (average salary of each name) → Last Layer is Dense layer with one output node ""UNK"": Unknown, out of vocabulary; characters not in Dictionary are defined as unknown Task in the end of Notebook 2: Classification problem → Changes to previous model (which was on regression problem) have to be made, concerning: Number of output nodes (As many nodes as classes) Metric used should be accuracy instead of MAE Loss function should be crossentropy instead of MSE    In general: Preparation of text data is essential  Lowercase all characters, otherwise data become sparse Tokenization: a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. Further processing is generally performed after a piece of text has been appropriately tokenized. Tokenization is also referred to as text segmentation or lexical analysis. You can define own rules fot splitting, f.e. by whitespace, punctuation.  ``` tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK') tk.fit_on_texts(train_data) word_index = tk.word_index sequences = tk.texts_to_sequences(train_data) data = pad_sequences(sequences, maxlen=SEQ_LEN, padding='post') Tokenize our training data We are using the TensorFlow (Keras) Tokenizer class to automate the tokenization of our training data. First we create the Tokenizer object, providing the maximum number of words to keep in our vocabulary after tokenization, as well as an out of vocabulary token to use for encoding test data words we have not come across in our training, without which these previously-unseen words would simply be dropped from our vocabulary and mysteriously unaccounted for.  fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. Get our training data word index A byproduct of the tokenization process is the creation of a word index, which maps words in our vocabulary to their numeric representation, a mapping which will be essential for encoding our sequences. Encode training data sentences into sequences Now that we have tokenized our data and have a word to numeric representation mapping of our vocabulary, let's use it to encode our sequences. Here, we are converting our text sentences from something like ""My name is Matthew,"" to something like ""6 8 2 19,"" where each of those numbers match up in the index to the corresponding words Get max training sequence length Pad the training sequences We need our encoded sequences to be of the same length. We just found out the length of the longest sequence, and will use that to pad all other sequences with extra '0's at the end ('post') - we could also truncate any sequences longer than maximum length from the end ('post') as well. - **TextVectorization**: This layer performs preprocessing of raw text: text normalization/standardization, tokenization, n-gram generation, and vocabulary indexing. - **How to**:      - Clean raw text data (within a function), f.e. you have to split word into characters with whitespace in between so that characters become like words in sentence     - Define a TextVectorization layer that will take the previously defined normalize function; define the shape of the output     - Call the vectorization layer adapt method to build the vocabulary     - The layer can be used in a Keras model just like any other layer tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=None standardize=LOWER_AND_STRIP_PUNCTUATION, split=SPLIT_ON_WHITESPACE, ngrams=None, output_mode=INT, output_sequence_length=None, pad_to_max_tokens=True, **kwargs) ``` if not everything has been understood so far....   Error Analysis Kaggle: https://www.kaggle.com/elitcohen/forest-cover-type-eda-modeling-error-analysis#Error-Analysis Content and goal of the Kaggle challenge: Prediction of forest cover types by certain location features General remarks which came up during exploring the Notebook:  Test and train data should have same distributions → otherwise it might impact your model Check distributions of target variable in test and train set after splitting the data If different distributions: Check for features which might influence shape of distributions and stratify on this feature In general:  Train-test-split picks observations randomly                 If you have unbalanced data concerning target variable, use stratify in order to get same distributions for target variable in train and test set.                  You can also stratify on features, which are important for your prediction                  (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)   If no test set available, split train set in train and validation set Used model is ExtraTreeClassifier, which is kind of a RandomForest Model with dropping out. Characteristics: Makes model robust to noise, since individual data points get less important Makes model less computational expensive    Error Analysis (EDA on Errors) in Kaggle Notebook: Steps included in the Kaggle Notebook: 1 Creation of Dataframe with all information from test set but only for the errors plus two additional columns True and Predicted 2 Printing out descriptives for error-Dataframe → Plotting would be a good idea here! 3 Confusion matrix   What to do now? - Find reaasons why classes are misclassified → Make assumptions - Check distribution of train and Error set - Do feature engineering based on this assumptions - Maybe create new features out of combinations of features - Maybe remove single classes Overview of Error Analysis Steps: https://slack-files.com/files-pri-safe/T019HBUU0QP-F01FWRKJPRN/1_error_checklist.pdf?c=1605876643-cd9d0f3ddb7e883c This was a classification task -  For regression problems, look at the residuals! Review on Error Analysis:  Plot distribution of errors against distribution of train data per feature: Should have same distributions if feature doesn´t affect size of error If there are deviations: you may bin data or - in our case - add binary features f.e. ""elevation"" > xy    Compare distributions of errors vs. train data for each class seperately Compare correctly classified ones vs. misclassified ones per class Maybe try PCA on errors Use combined features    Only looking at your error metric (accuracy, MSE etc.) is not sufficient: Look at details within a Error Analysis"
11,day31_20201109.md,Kristina Lin,1604957548,"Day 31 - Monday 9 November 2020 A social experiment  Protocol by Jakolien Schedule  09:00 Discussing Chandra's protocol about Day 30 9:30 Introduction to the take me home challenges 10:30 Working on challenge 13:00 10-minute one-on-two progress discussions 13:00 Working on challenge 16:00 Daily standup  Take me home challenge Run an EDA for a job application. Deliverables: * a readme that documents the thought process. * a notebook of the EDA. The notebook should look good and be self-explanatory: you will not have a chance to explain your code when your future employer looks at it. Your thought process needs to be clearly documented. The assignment had four domains: * Data analyst: Jenny, Sina, Chandra, Michael * Product analyst: Andrew, Kevin, Jakolien * Software engineering: Andreas, Ole * Data scientist: Laura, Martina, Petra, Sebastian Some tips and tricks: * If the stakeholder asks something impossible, e.g. does not supply enough data for the desired analysis, one has to report that and do a different analysis with the data. * If the application does not explicitly say that you may submit a Jupyter Notebook, submit the notebook as a HTML file. Jupyter Notebook can be exported as a HTML file.   * File -> Export -> Export Notebook as HTML * Always add your working environment to your application, so your code can be run by your future employer. * Instead of pivot-tables, one can use pd.crosstabs. * Boolean can be directly converted to integers. * If you cannot run your environment in jupyter lab:   * Add Jupyter Lab (jupyterlab) to the conda install for your environment.   * Add to the environment: conda install nb_conda     * Lets you choose your environment in Jupyter Lab. If you have trouble coming up with business cases or to look at a problem like a stakeholder would: * learn more about key performance indicators (KPI's). * read a book on how stakeholders think suggested by Andrew: https://www.amazon.com/Competing-Against-Luck-Innovation-Customer/dp/0062435612 Our solutions have been uploaded in the google drive linked below: https://drive.google.com/drive/folders/1DWq5-EqriSPEsoGQFnLGpdGMwYAGh3pi?usp=sharing Prepare for tomorrows code review session by looking at the solutions of your colleagues. Topic for the rest of the week: unsupervised learning."
12,day19_20201022.md,Scott Small,1603386087,"Day 19 - Thursday, 22nd October 2020 Identify the KNN - the most similar neighbors  Protocol by Jenny Schedule  9:00 - 9:40 Daily review 10:00 - 11:30 Fun with kepler 11:30 - 12:30 Lecture by Tereza & Mia on KNN and Distance Metrics 13:30 - 16:00 Work on jupyter notebooks on KNN and Distance Metrics 16:00 - 16:30 Daily standup  Fun with kepler  Playing around with Kepler and the kingcounty data set Kelper is a powerful powerful web-based geospatial data analysis tool, designed for large-scale data sets (https://kepler.gl/) Team-presentations on the results   Lecture by Tereza & Mia on KNN and Distance Metrics What is K-Nearest Neighbors (KNN)?  The idea of KNN is that similar things exist in close proximity (similarity).  Aka: Tell me about your neighbors, and I tell you who you are. You like red light, parties & beer? Probably you live in St. Pauli.  KNN is a supervised learning algorithm used to solve both regression and classification problems  How KNN works  (1) Load the data. (2) Set K to indicate the chosen number of neighbors. (3) For getting the predicted classes, iterate through all training data points:  (a) Calculate the distance between each row of the training data and the test data, using a distance metric of your choice.  (b) Add the calculated distance and the index of each data point to a list (or array or series). (4) Sort the list from the smallest to the largest distance (ascending order of distances). (5) Get the top k rows from the sorted list to set the selected classes. (6) Get the labels of the classes of these k slected neighbors. (7) If regression, return the mean or median class of the K labels (=predicted class). If classification, return the mode (most fequent class) of the K labels (=predicted class).  Useful link on how to implement with sckit-learn: https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/  How to implement KNN in python  (1) Import data. (2) Set K to indicate the chosen number of neighbors. (3) Define function of the distance metric that calculates the distance between two data points.  (4) Define function of KNN model (fetches the most frequent class of the neigbhors).  (5) Split data into training and test sets (6) Now use the distance metric function defined in (2) to calculate the distances between each row of the training data and the test data.  (a) Save the calculated distances and the indexes of the data points to a list (or array or series). (b) Sort the list from the smallest to the largest distance (ascending order of distances) (c) Extract top k neigbors from the list (7) Calculate the most frequent class of the k chosen neighbors using the KNN function defined in (4) on the training data (8) Return predicted class for y using the test data.   KNN summary  non-parametric Algorithm: no underlying assumption on data distribution classification based on similarity measures Lazy Algorithm → needs no training (all training data is needed in prediction phase) training time very short prediction time very high (depending on number of observations)  Pros and Cons of KNN  Pros Easy Implementation Gives good results for large sample sizes Highly adaptive due to usage of local information (non-parametric approach)  Versatile ( regression, classification, search)   Cons  All training data needs to be stored → large storage requirements Computationally intensive prediction phase. This is because in large datasets the cost of calculating distance between new point and each existing point becomes higher. Gives only good result for dense data → highly susceptible to the curse of dimensionality Lack of interpretability. The KNN algorithm doesn't work well with categorical features, since it is difficult to find the distance between dimensions with categorical features.  Hyperparameters - What influences the performance of KNN?  (1) The number of neighbors (K) which is taken to classify. (Danger of over- and underfitting). Find the best value for K by festing different numbers for K. Take K with lowest cost function on test data set.  (2) The distance metric used to determine the nearest neighbors.  (3) The decision rule used to classify the test data.  Distance metrics Common distance metrics  (1) Euclidean distance: Distance between two points is the length of the path connecting them.   (2) Manhattan distance: Absolute sum of the difference between the x-coordinates and y-coordinates.   (3) Minkowski distance  Generalized form distance.   Overview    When to use which distance metric?  Euclidean distance (Minkowski p=2) (L2-norm): most commonly used, when data is dense or continuous, this is the best proximity measure. Manhattan distance (Minkowski p=1), other names are L1-Norm, Taxicab or City-Block distance:. Better for sparse data (high dimensional data). → Whenever Distance Metrics are used, keep in mind to scale the data  Further applications  Further applications of distance metrics are... K-means clustering NLP (=natural language processing) "
13,day2-20200929.md,Joy Williams,1601536723,"Day 2 - Tuesday, 29 September 2020 Introduction of the Software Used During the Bootcamp :exploding_head: Protocol by Andreas Standard Schedule  9 - 10: daily review going through the protocol discussing and answering open questions 10 - 12: class Rapid assessment test (RAT): one person will have to give a presentation of the things we learned and answer a few questions 12 - 13: lunch break 13 - 16/18: class end of class Daily standup: What hav I learned today? What will we do tomororow? Are there any blockers? End of day questions (EOD): Three questions concerning todays class, link in Slack, answer on google docs  Computer in General  if you have a problem Have you tried turning it off and on again? kernel: core of the computer system allocates memory and processor time  Slack  colaboration tool, virtual class room with chat, messenger app channels  becoming-data-scientist: for course-related topics   capstone: capstone-project that we will work on in the last four weeks of the bootcamp   general: announcements from neuefische   meeting-links: here the zoom-links for classes and breakout rooms will be shared   random-stuff: anything that is not course related, e.g. pictures, jokes   slides: here you can find all the slides we've already worked through   Zoom  virtual meeting room links shared in Slack's #meeting-links channel  Terminal  other names: console, command line, prompt, shell, bash working with the terminal is faster than using a graphic interface ""Using a graphic interface is like swimming with your head out of the water, instead of under water"" opened with Finder: Applications\Utilities\Terminal.app Spotlight: press command + space, enter terminal Spotlight: click on the magnifying glass in the upper right corner of the screen, enter terminal helpful shortcuts:  Keys | Effect -----|------- command + t | open a new terminal tab tab | activate autocomplete and displays a suggestion arrow right | complete suggestions arrow up | get back previous commands  important commands:  Command | Meaning | Effect --------|---------|------- pwd | present working directory | displays the name of the folder/directory you are currently in ls | list | displays all files in the current folder/directory ls -la | list | displays all files in the current folder/directory including hidden files (filenames of hidden files start with a .) cd [name in current directory] | change directory | change directory to one of the folders in the current directory cd .. | change directory | change directory to the next higher folder cd ~ | change directory | change directory to the home directory cd [path] | change directory | change directory directly to a specific folder, e.g. ~\neuefische\stuff mkdir [name] | make directory | create a new folder in the current directory cat [filename] | concatenate | displays contents of the file, only use for small files vim [filename] | vim texteditor | opens a file in the vim texteditor  manipulating files with the texteditor vim press i for insert mode: type like normal text editor press esc for command mode: give commands to the editor to get things done a collection of vim-commands can be found here: https://coderwall.com/p/adv71w/basic-vim-commands-for-getting-started useful commands in command mode:  Command | Effect  ---------|------- dd | delete the line :wq | save and exit :q! | exit without saving  customizing the terminal iTerm2: an alternative to standard Mac-Terminal Z-Shell: a command-line interpreter, adds color to the temrinal, standard without Z-Shell is ""bash"" Oh My Zsh: allows customizing the appearance of the terminal  GitHub  platform to share code and for version control project-repository also valuable for job applications project-repository accessable from terminal by creating SSH-Key in terminal and copying it to GitHub in browser ""fork"" copy public code to your own online project-repository ""cloning"" downloading code from your online project-repository onto your computer ""pushing code"" uploading code protocolls can be written directly here create a new .md-file in hh-2020-ds4-daily-review after finishing send a ""pull request"" share the link in Slack's #becomming-data-scientist channel  Anaconda  Anaconda is a package management system the combination of the Python version and all used packages for a specific project are called environment environments allow long term access to projects because all necessary packages are installed in the right version they allow parallel work on multiple projects that use different versions of Python or individual packages the command conda env create -f [name].yml creates a new environment the environment we created and will use in our class is called ""nf_simple"" conda env list displays all available environments, the current environment is marked with * conda activate [environmentname] activates the environment the .yml file can be viewed with any text editor (e.g. cat , vim) alternatively a new environment can be created directly with conda create --name [environmentname] create the .yml file with conda env export > [name].yml to install packages better use conda install [packagename] instead of pip install [packagename] after installing new packages, the .yml file must be exported again to be updated helpful commands can be found here: https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf  Jupyter Notebook  helps organizing, implementing and documenting the work do not push a GitHub-repository without a readme, usually written in Markdown open via terminal with jupyter lab open a file directly in Jupyter Notebook with jupyter lab [filename] essential components of a notebook: title, menu, cells Jupyter has a dual mode: edit vs. command mode edit: cell is green bordered, cell content can be edited, switch cell type between code and markdown, use esc + m to turn a cell into a markdown cell and esc + y to turn it into a code cell command: cell has a blue border, cells can be deleted, added etc. before giving the notebook to another person, re-run completely important commands:  Command | Effect --------|------- shift + enter | execute current cell command + s | save a | insert cell above (only command mode) b | insert cell below (only command mode) dd | delete cell (only command mode) h | show all shortcuts (only command mode)  more shortcuts here: https://towardsdatascience.com/jypyter-notebook-shortcuts-bf0101a98330 find the Markdown tutorial ""Mastering Markdown"" here: https://guides.github.com/features/mastering-markdown/ "
14,day37_20201117.md,Kristina Norris,1605647266,"Day 37 - Tuesday, 17th November 2020 Protocol by Sebastian Convolutional Neural Networks (CNN)   Schedule  09:00 - 09:30: Daily review - Artificial Neural Networks 09:30 - 11:00: Working on branch 3 notebook 3 11:00 - 12:45: Lecture: Image Modelling 12:45 - 13:45: Lunch break 13:45 - 14:00: Git Branch Commands 14:00 - 16:00: Working on Branch 3 Notebooks 1-2 16:00 - 17:00: Daily check out   Q&A on previous notebooks  How do we interpret the tensorflow summary:    Number of parameters = number of filters x kernel size x depth of the previous layer + number of filters (for the biases): input_1: This is a place holder and has no trainable parameters conv2d:               Number of filters = 32, kernel size = 3 x 3 = 9, depth of previous layer = 1, so 32 x 9 + 32 = 320 max_pooling2d:        Max pooling layers have no trainable parameters. The conv2d_1:         Number of filters = 32, kernel size = 3 x 3 = 9, depth of previous layer = 14 so 32 x 9 x 32 + 32 = 9,248 conv_2d_2, conv2d_3:  Same as conv2d_1 conv2d_4:             1 x 9 x 32 ...    Classification Notebook  Estimators in Tensor Flow use Feature Columns to describe how the model should interpret each of the raw input features can be raw input features (numerical or categorical) can be new created colums (derived feature columns)  they provide some feature engineering capabilities like one-hot-encoding, normalization, and bucketization   Modelling:  Initialize and train the model linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns) linear_est.train(train_input_fn)   Evaluate model and get all the scores you need: linear_est.evaluate(test_input_fn) accuracy baseline is 0.5 for perfectly balanced data     Lecture: Image Modelling 1. Basics: + We use Convolutonal Neural Networks (CNN) for image classification   + e.g. modern mobile phones do face recognition to store and categorize your photos  Image specifications: Height(Pixels) x Width(Pixels) x Depth(Channels) Channels: 1 for Black&White, 3 For RGB (RedGreenBlue) Compute size for a Photo(RGB): 1280 x 720 x 3 = 2.764.800 pixel or 2.76mb  Pixel Intensity Range: Every pixel (regardless if its colored or not) has a range from 0 - 255   Images are unstructured data  Images can be flipped, rotated, scaled, translated or seen from a different angle therefore we are rather interested in pixel relations  Ideal Feature Information:  transformation is done by flattening a 2 dimensional tensor into a 1 dimensional one we can then process the data like structured data    Models in Neural Networks:  Linear Logistic Regression Model, like on MNIST Fashion Dataset, work but have room for improvement (Accuracy 86%)   DNN perform better on Image Data (91% Accuray) but also have disadvantages (overfittin    2. Convolutional Neural Networks  they first appeared after an ImageNet Competition in 2012 original winners of 2011 got outperformed by a convolutional neural network a year later consists of Convolutional layers and Pooling layers  Overview  Convolutional Layers + convolution as grouping information + they use a special mathematical method to capture relationships between pixels + How it works:     + a filter (kernel) strides over the image pixels (with pixel intensity 0-255) and creates feature maps      + the combined feature maps create a convolutional layer with n - kernels (filters)     + we can train filter values like models + a convolutional layer reduces the dimensions of an image   + to reduce shrinkage we can add framing (in practice commonly 0)   + formula:      +      m = n + 2p - k + 1,  with:      +      m : resulting image dimension     +      n : picture side length (28 for an 28x28)     +      p : padding, framing     +      k : kernel side length (3 for an 3x3) + the convolution takes place across channels, so for a RGB Image it's a cube + we can share parameters to save a lot of memory (compared to DNN) Pooling + Pooling is aggregating features from feature maps and reduces the feature maps dimensions    + formula:     +      m = (n + 2p - k)/s + 1,   with:     +      s = pooling layer side length (2 for a 2x2)  Pooling has two major effects: a. It makes the network invariant to small translations  when a signal occurs in different locations of a feature map area it activates the next neuron in any case cross-channel pooling allows invariance to other transformations (like rotation) b. It strengthens the signal while it is running through the network  Trainign the Modell   Stack layers together with a softmax or sigmoid layer at the end  Train with gradient descent to optimize parameters Feature Extraction: hierarchical order feature complexity grows in each layer (simple structures like edges first) Data Augmentation and Transfer Lerning can help with data hungry Deep CNNs: DA: roates,translates (...) image randomly DA: enlarges dataset but makes it more robust TL: Artificial Neural Networks can learn tasks by transferring knowledge from tasks they learnt before    GitHub Terminal Codealong | Command | Description |   | ------- | ----------- |   | git checkout main | Switch branches |   | git pull | Get changes from remote repo |   | git remote -v | Check remote repository for new branches |   | git remote add upstream https://github.com/neuefische/2020-ds-intro-to-tensorflow.git | Add link to new branch from remote main branch |   | git remote -v | Check remote repository for new branches  |   | git branch -a | Displays all branches |   | git fetch upstream | fetch new branch from remote main branch |   | git status | Check Status of your repo |   | git checkout second-image-classification | check if new branch is ready to be tracked from upstream |   | git checkout main | Switch to main branch |   | git merge upstream/main | Merge new remote branch into the local main branch |   | git switch second-image-classification | Switch to new branch |   | git push origin second-image-classification | Push Changes from local branch to remote personal repo |   HTML FUN WITH MICHAEL: "
15,day38_20201118.md,Kelsey Joseph,1605730027,"Day 38 - Tuesday, 18th November 2020 Protocol by Petra Notebooks / Capstone / Job Application Training   Schedule  09:00 - 09:30: Daily review - Neural Networks 09:30 - 12:00: Finish notebooks 12:00 - 13:00: Lunch break 13:00 - 14:00: Capstone Q&A Fist steps 14:00 - 16:00: Capstone groupwork 16:00 - 18:00: Job Application Training with Anne   Notebook   Q&A:  Regularization: add to convolutionary layer and dese layer do not in the input, output and pooling layer drop out also in convolutionary and dense layer with different percentages Open question: Where and how often? Flatten before Output to reduce from  LeNet: How to ""remove"" connections? Tereza: this was probably drop-out, but not randomly but connection with small weights Number of filters: look for existing and well functioning networks like the LeNet Example pick always a number with the power of 2 (32, 62, 128 etc.)  https://github.com/mjbhobe/dl-tensorflow-keras/blob/master/Fashion%20-%20CNN%20-%20Keras.ipynb https://towardsdatascience.com/how-to-improve-a-neural-network-with-regularization-8a18ecda9fe3 Q&A Capstone   First steps:  Define the objectives, questions and assumptions What does the stakeholder want to know? What do you want to predict?   Timebox your project:  Make clear plans and define deliverables for each week!   What are your final deliverables?  What do you want to present to your stakeholder? How do you want to visualize your results?       EDA:  Have a clear goal in mind and do not get lost    Capstone Meetings:  ""Stakeholder""-Meeting with Mia and Tereza after 2 weeks Google sheet with progress in each week   Miro for brainstorming can help to visualize ideas  Job Application Training   neuefische cooperation with companies:  if you are interested in working for one specific company (in the pond or others), you can contact Karolina and ask if she can invite the company to the capstone presentations    CV:  Clearr overview abaout steps in your (career)life: Guiding questions: Where do I come from? Where am I? Where do I want to go? What are my skills? Who am I?   Structure and diligence Antichronilogical Important Keywords Watch out for discrimination!! (Age, Children, Religion, Politics etc) Short summary to explain your career change Include grade (if very good) Do not mention your skills ""randomly"", make a ranking or inlcude grading References can be included, but better to include projects than people (profs are ok though) Make the CV specific to job and company Do not use standard templates (XING, Google) e.g. look at etsy, canvas, cakeresume   Interests: No dangerous sports, no politics, no topics that could cause discriminating     Cover letter:  Do not begin with a standard sentence, start with yourself, ""Hello, as a data science bootcamp student.."" 3-4 sentences about yourself (nothing that is already mentioned in your cv) 2-3 sentences why you are intersted in the job/company 1 sentence that you would look forward to a first interview Cover letter is not a motivation letter. Not so important anymore. If possible send/upload only your CV 1on1 CV check with Anne: Please send your updated CV till 25.11. anne@neuefische.de    Job interview:  Relax!! Talk! but not too much (60%) Preparation is key Be confident: talk about what you can and what you acheived, if you don't know something just say that you are interested in learning this Talk about your projects at neuefische    Salary:  45.000 - 50.000 for a junior postition    Websites:  Google Jobs, Indeed, LinkedIN but: always apply directly on company website    Capstone event:  Zoom meeting with presentations Breakout rooms for further discussions with the companies "
16,day17_20201020.md,Shawn Vega,1603290137,"Day 17 - Tuseday, 20th October 2020 Mathematical Mind  Protocol by Chandra Schedule  09:00 - 09:45 Daily review 10:00 - 10:30 clarification on some topics / issues from Tereza   10:30 - 12:00 Presentation by Tereza about Logistic Regression  13:00 - 16:00 Work on notebooks 16:00 - 16:45 Daily standup  question and answer  Notebook 1: Confusion Matrix quick Demo     Question 1: why we have 0 for negative in clasification report ? clarified by andrew : intepretation of 0 and 1 is more general in nature   Question 2: what is weighted average ?   Ans: weighted average =   support - number of actual events n - total number of samples   Question 3: Examples of situations where either High precision or High recall is required Recall : Gold mining, Car battery recharge notifier Precision : Duck hunting, picking random sticks while hiking     Tips from andreas about how to write Formulas in Markdown file  We went through group open questions For problem of mismatch between number of commits between Github and local repo Tereza gave us a link to rebasing, but was not succesful, below is the link     https://gist.github.com/ravibhure/a7e0918ff4937c9ea1c456698dcd58aa   Difference between squash and merge with only merge squash and merge will pack all commits into one before merging on to master only merge will merge all commits on to master  Logistic Regression  Examples where we use Logistic Regression    Email: spam vs not spam   Tumor classification: malignant vs benign   Bee image: healthy vs not healthy   Sentiment analysis: happy vs sad   Text analysis: toxic vs not toxic   Animal detection: cat vs dog vs monkey vs… ( multi class      classification)    Face recognition: iphone owner or not   Why use classification?    Applying linear regression to a classification problem is     usually not a great idea   First time we got lucky and we got a hypothesis that worked     well for the particular example Another problem with LR is y can be 1 or 0, but the     hypothesis h𝜃(x) can be larger than 1 or smaller than 0     even when all the training data was with 0 and 1   Logistic Regression Model |  Name        | Formula    |  | :------------- | :----------: |  |  Linear regression | hθ(x) = θTx    |  | Logistic Regression   |    hθ(x) = σ(θTx) | | Sigmoid Function   |    σ(t) = 1 / (1 + e-t)| Probability h𝜃(x) is the probability that y = 1 on given input x Example:     If h𝜃(x) = 0.8    the probability of y=1 given the feature vector x is 80%   Notation of probability:    h𝜃(x) = p(y=1|x; 𝜃) The decision boundary  We can set 0.5 as the threshold, which is also python default value   If the probability is bigger than or equal to 0.5 it is classified as 1, else 0   Classification will be based on probabilities with thresold as 0.5   If the probability is bigger than or equal to 0.5 it is classified as 1, 0 else   The decision boundary is a property of the hypothesis and not of the data   we fit the model to find the parameters 𝜃 and then we can get the decision boundary based on 𝜃   By using higher order polynomial features we get a non-linear decision boundary    Example of non-linear decision boundary   Cost Function  Cost Function quantifies the error between predicted values and expected values    and presents it in the form of a single real number  |  Name        | Formula    |  | :------------- | :----------: |  |  Logistic regression hypothesis |   hθ(x) = 1 / (1+e-θTx)    |  | Logistic Regression   |   | Loss based on logs of probabilities  For positive instances it is the negative log of the probability      For negative instances, the loss is the negative log of (1-probability)     Loss(θ):               (-log p^)     if y = 1               (-log(1-p^))  if y = 0   Maximum likelyhood estimate / Loss function    Notebooks on Logistic Regression  Notebook 1: Logistic_Regression_Codealong.ipynb Notebook 2: Logistic_Regression_Codealong2.ipynb Notebook 3: Logistic_Regression_Titanic.ipynb Notebook 4: Logistic_Regression_Exercise.ipynb  spoiler alert while dropping the columns  * As a rule of thump 50 records per feature   Reference material  Hands on machine learning Machine learning course by Andrew ng complex decision boundary https://www.researchgate.net/figure/Classification-results-of-SVC-with-the-nonlinear-decision-boundary_fig1_255572951    Don't forget the open questions  https://docs.google.com/document/d/1DdBqIdhwZoE8QxzyqsHqUsCD0cN9_zb03lBwnOzXcU0/edit?usp=sharing "
17,day16_20201019.md,Sarah Pace,1603095590,"Day 16 - Monday, 19th October 2020 Total Recall  Protocol by Andreas Schedule  9:00 - 9:30 Daily review 10:00 - 10:20 How not to update your Mac 10:20 - 10:35 King-County presentation by Laura Italian mafiosi selling several central houses (top10%) over time, needs average outskirt houses over time 10:35 - 10:55 King-County presentation by Jenny Investor buying historical houses in best neighborhoods, looks for best timing within a year, wants high profits 11:00 - 12:30 Lecture by Tereza about evaluation metrics 13:30 - 16:00 Work on notebooks 16:00 - 16:15 First information on capstone project 16:15 - 16:40 Daily standup  Last King-County presentations  General remarks Well structured Good use of color palette and maps Clear summary at the end for the stakeholder Also note that Graphics fit on one screen Graphics include a key / legend Median already resistant to outliers All notebooks can be found here: https://docs.google.com/spreadsheets/d/13nPA9zWuFaA4uCdvn-UA7-yPZZ-qpMdn4OgfIPw9x8Q/edit?usp=sharing If you want to present an improved version of your notebook, talk to Tereza  Evaluation Metrics  Validating model and inspecting how a model is performing Metric and thresholds should be chosen depending on customer needs before modelling After getting the evaluation results, look into the details! Different metrics for Regression and Classification  Classification  Evaluation      | The eMail in reality       | Your spam classifier says   | Example ---------------------|--------------------------------|---------------------------------|-------------  True Positive (TP)  | The eMail IS actually spam.    | Your filter says it IS spam.    | 100  False Positive (FP) | The eMail is actually NO spam. | Your filter says it IS spam.    |  10  False Negative (FN) | The eMail IS actually spam.    | Your filter says it is NO spam. |   5  True Negative (TN)  | The eMail is actually NO spam. | Your filter says it is NO spam. |  50 Name                  | Nickname        | Formula                 | Example ---------------------------|---------------------|-----------------------------|-------------------------------------  Accuracy                  |                     |  |   True Positive Rate (TPR)  | Recall, Sensitivity |  |   Precision                 |                     |  |   False positive rate (FPR) | Fallout             |  |   True negative rate        | Specificity         |  |   False positive rate (FPR) + Specificity = 1 F1-score Harmonic mean of precision and recall  Choosing the threshold (in a binary Variable ""Tweaking"") is important High threshold gives higher Precision, but lower Recall Receiver Operating Characteristic (ROC curve) Area Under the Curve (AUC)   Regression  Mean Absolute Error (MAE) Mean Squared Error (MSE) R-squared ()  Notebooks on evaluation metrics  Notebook 1: Confusion matrix quick demo https://machinelearningmastery.com/confusion-matrix-machine-learning/ http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/ Notebook 2: Understanding data science classification metrics in Scikit-Learn Notebook 3: Regression metrics from scratch  Capstone Projekt  Project can also be done in teams of two Start looking for data soon, e.g. On Kaggle Companies you want to work for Ask the companies early Template available from neuefische Asking a company is a good practice for job applications and the real job as a data scientist   neuefische also has data from cooperating companies Transcripts of TV shows https://typingdna.devpost.com ...  Don't forget the open questions  https://docs.google.com/document/d/1DdBqIdhwZoE8QxzyqsHqUsCD0cN9_zb03lBwnOzXcU0/edit?usp=sharing "
18,day27_20201103.md,Cole Schultz,1604427455,"Day 27 - Tuesday, 03 November 2020  Protocol by Sina Schedule   9:00 - 09:15 Daily review of day 26   09:15 - 12:00: Work on second project   12:00 - 13:00: Lunch Break   13:00 - 13:30: Q&A 1    13:30 - 16:30: Work on second project    16:30 - 17:30: Q&A 2   17:30 - Open end: Work on second project (or not)    Q&A 1  Should model focus on small bias or generalizability? <<<<<<< HEAD* Dependent on business case!  Dependent on business case!       upstream/master         For Data Analysis (focus on the past): Generalizability not main focus For making predictions: Generalizability has to be taken into account     <<<<<<< HEAD Q&A 2 How to build a model in python script  ======= Q&A 2 How to build a model in python script        upstream/master Link:  https://github.com/neuefische/ds-modeling-pipeline/tree/first-pipeline  * File_1: Train file:     * Import necessary libraries     * Read data     * Define X, y     * Splitting data into train and test sets     * Save X_test and y_test in data folder     * Apply feature engineering functions to Train Data (out of file_2)     * Create model (fit&train) and check performance metrics <<<<<<< HEAD     * Apply feature engineering functions to Test Data (out of file_2) and check performance metrics     * Save model as pickle:         ```filename = 'models/linear_regression_model.sav' ``` ```pickle.dump(reg, open(filename, 'wb'))```  =======     * Save model as pickle:  filename = 'models/linear_regression_model.sav' pickle.dump(reg, open(filename, 'wb'))        upstream/master * File_2: Feature engineering:     * Import necessary libraries     * Define functions for cleaning your data (f.e. handling missing values, dropping outliers etc.)          File_3: Prediction:  Import necessary libraries Call the paths of your model, X_test and y_test. This is done by using sys.argv. sys.argv is a list in Python, which contains the command-line arguments passed to the script: f.e. python predict.py path_model, path_X_test, path_y_test By indexing, you can assign those arguments to variables    Load model from disk:  loaded_model = pickle.load(open(model, 'rb')) Apply feature engineering functions to Test Data (out of file_2) and check performance metrics    gitignore file:  add data folder and models folder: data/*.csv models/*.sav    <<<<<<< HEAD * init.py file:     * Files name init.py are used to mark directories on disk as Python package directories. Is needed so that scripts can read each other =======   __init__.py file:  Files name __init__.py are used to mark directories on disk as Python package directories. Is needed so that scripts can read each other.       upstream/master            General hints:  When importing libraries, do it in the following order: Import core libraries, imported libraries and in the end, own functions    Dealing with imbalanced data <<<<<<< HEAD Link: ======= Link: https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18        upstream/master       "
19,day24_29102020.md,Ellen Henderson,1604014506,"Day 24 - Monday, 29 October 2020 More ensemble  Protocol by Ole Schedule   9- 10: Daily review of day 23 (insert brain leaving skull here)   10 - 10:30: Exercise: Share your strengths and weaknesses   Offer from Tereza to everyone to give a talk on how to improve on ones' own weaknesses   10:30-12: Pair programming: Working on Notebooks    12 - 13: Lunch break   13 - 14:45: Pair programming: Working on Notebooks    14:45 - 15:15: First look at project 2   15:15 - 15:30: Daily check out   15:30 - 16:30: Pair programming: Working on Notebooks    Teaser for second project  4 data sets to choose from Groups pre-assigned  Cases to choose, instructions etc. can be found at --> https://docs.google.com/document/d/1WBgIIiripdVgKLxC8WRwMjPUMVkPDV90ew5G0_elZ44/edit#heading=h.ig9xxmkvyui3 Tips & advice for second project  First step: Think of and define your business case! What is important? High accuracy? Low false positives? Pre-process the data Regardless of whether it is a regression or a classification problem, choose a ""light"" model first - for example LinearRegression, Naive Bayes  Notebooks More ensemble models Notebook 4 from day 23: Ensemble models (passive) Notebook 1: Applying XGBoost  Note: To install the xgboost package, use ""conda install -c conda-forge xgboost"" while you have the conda environment of your choice already activated (for example: nf_simple)  Notebook 2: AdaBoost Codealong (passive)  Developing your own Adaboost algorithm  Notebook 3: Comparison classification algorithms  Whole EDA & data pre-processing journey (good blueprint for second project?) Comparison of Decision Tree, Support Vector Machine, AdaBoost SVM already slow on >30k samples (103 features) AdaBoost performs best Feature selection Feature importance is based on improvement on R^2 and/or the number of times it was included in a (good) predictor "
20,day22_20201027.md,Rebecca Butler,1603822177,"Day 22 - Tuesday, 27th October 2020 SVM - Support Vector Machine Protocol by Martina Schedule  9:00 - 9:30 Daily review 9:30 - 12:00 Groupwork on SVM 13:00 - 14:15 Presentations 14:15 - 16:00 Working on notebooks  Daily review  When do I have to remove outliers?   1) first split the data   2) CV to test train data with 0, outliers   3) remove outliers which are by mistake (no numbers that might be true) and run again the CV; if the outliers are random -> mean (no outliers) or median (outliers)   4) CD: apply test data ! Don´t clean test set, because we don´t know the test set (e.g. kaggle) you can apply functions for cleaning the train data and then apply it to the test data  Links  scikit-learn algorithm cheat sheet: https://d6-neuefischedatahh.slack.com/files/U01BW1219H7/F01DDP00L6P/ml_map.png capstone: https://www.drivendata.org/ ____ https://geoportal-hamburg.de/udp-cockpit/#/  Groupwork on SVM What is it about?  SVM is a supervised machine learning algorithm used for both classification or regression problems Vocabulary: hyperplane: decision plane which separates the data into different classes support vectors: points which are closest to the hyperplane margin: gap between the two lines on the closest class points goal: Find the best hyperplane which segregates the classes in the best way.   How does the algorithm work?  * find the points closest to the line from both classes (points called support vectors) * compute the distance between the line and the support vectors (distance called margin) * maximize the margin: The hyperplane/line for which the margin is maximum is the optimal one Kernel Trick  if the data is not linearly seperable we have to transform the problem into a higher dimension because there we get a linear classification (we have to find the optimal decision boundary) to compute the dot products of vectors in the higher dimension, we don’t actually have to project the data into higher dimension we use the kernel function to compute the dot product directly using the lower dimension vectors (more efficient and less expensive) functions: linear kernel, polynomial kernel, radial basis function (RBF) kernel, sigmoid kernel    Tuning parameters  C:  trade off between smooth decision boundary and classifying training points correctly  a large C means you will get more training points correctly try different values of C for your dataset to get the perfectly balanced curve and avoid overfitting  for very large  C the margin is hard, and points cannot lie in it  for smaller C the margin is softer and can grow to encompass some points   Gamma:  defines how far the influence of a single training point reaches if gamma has a very high value, the decision boundary is just going to be dependent upon the points that are very close to the line which effectively results in ignoring some of the points that are very far from the decision boundary shape of boundary line high gamma more curvature  low gamma less curvature   Gridsearch: defining parameter grid  param_grid = [{'kernel': ['rbf'], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1], 'C': [1, 10, 100, 1000]},                           {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}] grid = GridSearchCV(clf_lin, param_grid, refit = True, verbose = 0)  Implementation in Python  from sklearn.svm  import SVC clf = SVC(kernel='linear'/'polynomial'/'rbf') https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC  Advantages  easy to use can solve classification and regression problems effective in high dimensionsal spaces for linearly and non-linearly seperable data different kernels which also can be combined to achieve complex hyperplanes  Disadvantages  not suitable for large data sets features have to be scaled results are strongly dependent on a suitable choice for the softening parameter C results do not have a direct probabilistic interpretation  Applications of the algorithm  Bioinformatics Face detection Classification of images Text Categorization Geo and Environmental sciences  Notebooks Repo: https://github.com/neuefische/2020-ds-SVM 1. SVM_Cancer 2. SVM_2 Presentations https://github.com/neuefische/hh-2020-ds4-daily-review/tree/master/support_vector_machines"
21,day15_20201016.md,Joy Williams,1603047552,"Day 15 - Friday, 16th October 2020 Learning to Love Statistics Protocol by Andrew Today's Schedule  9 - 10: daily review of day 14 Tips and Insights into EDA and modelling Feature selection != Feature Engineering. Feature selection is selecting the variables / columns / 'X's to be used in the model. Feature Engineering is creating new variables by transforming and combining existing variables in new ways. Outlier removal can be done on all data during EDA but when it comes to modelling the splitting into test and train sets should be done prior to cleaning so that there is no cleaning performed on the test data. Gauss Markov assumptions can be used as a guide to determine good models.   Conda Channels: Installing packages from Conda Forge: There are some packages that are developed and published in 'channels' that are not part of the main conda library repository. These can be installed similarly to standard packages by using the command:     conda install -c conda-forge <package-name>  More info on Conda-Forge here: https://conda-forge.org/   All King County EDA repos have been published to github and can be found here: https://docs.google.com/spreadsheets/d/13nPA9zWuFaA4uCdvn-UA7-yPZZ-qpMdn4OgfIPw9x8Q/edit?usp=sharing   10 - 12:00: Lecture: 12 months of Statistics in 120 minutes  Bias and Variance Underfitting vs Overfitting Regularization 12 - 13: lunch break 13 - 16:00: Pair programming to work through the exercises on Linear and Polynomial Regression. 16 - 16:30: Check in with Mia 16:30 - EOD: Finalise notebooks end of class  Highlights of Statistics Lecture: Bias-Variance-Trade Off Check out 'StatQuest' on youtube for easy to understand explainations of statistical and Data science concepts: https://www.youtube.com/user/joshstarmer CONCEPTS: * Linear Regression: See previous lectures. * Estimators: $\theta$ $\hat{theta} = \sigma(D)$ An “estimator” or point estimate is a statistic ( a function of the data) that is used to infer the value of an unknown parameter in a statistical model  Mean Square Error:  Mean Square Error is defined as the expected value of the squared errors. E - expected value of a random variable is a generalization of the weighted average Expected Value:  The expected value of a random variable X is a generalization of the weighted average, and is intuitively the arithmetic mean of a large number of independent realizations of X.  Bias: Is related to the errors of a model and comes from underfitting. Variance: Comes from errors resulting from model fit and is a consequence of overfitting.  In order to avoid 'Bias', don't always try to create the 'best' model based on the data you have. This means you should not 'overfit' the model to the specific data. The model should also be generalizable. Low variance means the model performs well across different data sets (different samples from the population). Experiments should be used to generate a model that has similar error across train and test data.  Overfit: Model is too complex, doesn't generalise. This can be improved through regularisation or more data.  Underfit: The model isn't very good at predicting, bring a better model, perform feature engineering, ease regularisation parameters.   Test / Train Split: Act of splitting the dataset in order to be able to evaluate model on unseen data.   Loss Function: An optimization problem ((e.g. fitting a model by linear regression) seeks to minimize a loss function. Therefore a Function that is used in an optimization problem to tune the model is a 'loss function'. For example MSE seeks to minimize the square errors of the model.  Regularisation: Introduces new elements to the loss function that penalise complex models in an effort to prevent overfitting. Will reduce variance and increase bias. L1 Lasso (absolute value of coefficients, Diamond shaped space). L2 Ridge (Square of Coefficients, Circular space). Grid Search method used to find optimal solutions. A brute force algorithm. Elastic net hybrid using both ridge and lasso terms  Repo 2020-ds-Polynomial-Regression Notebook 1: Polynomial Regression file: 1-Polynomial_Regression.ipynb  Based on this blog in Towards Data Science:: https://towardsdatascience.com/polynomial-regression-bbe8b9d97491     Introduction to creating higher order equations for models. Using Powers of original features extends the linear model but they remain linear because the coefficients are still linear even though the curves are polynomial. Use the PolynomialFeatures class provided by scikit-learn:  ``` import operator from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures ``` Notebook 2: Polynomial Regression file: 2-Polynomial_Regression_Codealong_Toward_Datascience.ipynb Based on this blog in Towards Data Science: https://towardsdatascience.com/machine-learning-polynomial-regression-with-python-5328e4e8a386    Practice using the Linear Regression package by recreating the blog steps. Notebook 3: Boston Housing Exercise - Polynomial Regression file: 3-Polynomial_Regression_Boston_Housing_Exercise.ipynb Dataset is available from sklearn datasets: from sklearn.datasets import load_boston boston_dataset = load_boston() data = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names) data['MEDV'] = boston_dataset.target More Practice building linear models and polynomial features. If your chart of the model line is bouncing all over the place then the data needs to be sorted"
22,day8_20201007.md,Tammy Sexton,1602057784,"Day 8 - Wednesday, 7th October 2020 Introduction to Pandas Protocol by Laura Today's Schedule  9 - 10: daily review of day 7 To implement doctstrings use three quatation marks e.g. """"""type here"""""" def function(argument: str)-> int:tells you the input is a string and the output output of the function is an integer, it is for the user's knowledge to use functions from other files you need to import them e.g. from rpn_calculate.py import calculate lamda functions - can be used locally as an argument lambda input1, input2: body of function e.g. lambda a, b: a+b    file extensions Extension | Type of file -----|------- .md | markdown .py | python .ipynb | jupyter notebook/lab   .pop() takes the last element, deletes it and returns it  if index is added in the brackets it ""pops"" the element at that index    stacks: is a linear data structure that stores items in a Last-In/First-Out (LIFO) or First-In/Last-Out (FILO) manner  a new element is added at one end and an element is removed from that end only from collecitons import deque     10 - 10:30: Introduction to the day's activities  Task for the day: work through the condelong notebooks in the repository 2020-ds-Pandas (https://github.com/neuefische/2020-ds-Pandas) 12 - 13: lunch break 13 - 13:30: Check in with Mia and Tereza 13:30 - 16: Continue pair programming to work through the assigned tasks 16 - 16:30: Answer any outstanding questions and organisational topics Daily standup: What have I learned today? What will we do tomororow? Are there any blockers? No End of day questions (EOD) end of class  Repo Pandas 1 Notebook 1: Intro to Pandas file: 1-Intro-to-Pandas.ipynb  to unzip the data.zip folder you can either double click on it in Finder or type unzip data.zip into terminal to get access to everything in the pandas library you need to import it: import pandas as pd pd is the standard way to import Pandas library  Command | Explanation | Example  -----|------- |------- pd.DataFrame(data_list) | passes data already present in our Python program into the DataFrame contructor pd.DataFrame(data = data_vals, columns = data_cols) | if the data and the column names are in two seperate lists we can pass both of them as arguments (have to make sure that the greatest number of elements in any single list corresponds to the number of column names) pd.read_{data_type}(""file_name"", delimiter) | allows us to read external data into a Pandas DataFrame (https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html), default for delimiter in csv is "",""  | df = pd.read_csv(""my_data.csv"", delimiter: "";"") pd.read_{data_type}(""file_name"", header = None, names=[""col1"", ""col2"", ...) | if the file has no column names in the first row they can be added, the default assigns numbers, namesallows us to hold a list with the names we want to assign to the columns df.shape | gives us the number of rows and number of columns df.columns | gives us back a list of all of the column names df.info() | allows you to look at the data type of each column and the number of null values df.describe() | gives you summary statistics for all of your numeric columns df.head() | shows you the first n columns (default n=5) df.tail() | shows you the last n columns (default n=5) df.column_nameor df[""column_name""] | grabs all the information in that column (dot notation only works if column name has no spaces), can also access multiple columns by passing a list of column names  | df[[""column_name1"", ""column_name2""]] cols = df2.columns.tolist(), cols = [col.replace(' ', '_') for col in cols], df2.columns = cols | this allows you to rename columns to get rid of any spaces df.copy() | makes a copy of the dataframe which can be stored if saved to a new variable df[start:end]| allows you to grab a subset of rows (needs a starting and ending index, i.e. can not pass a single index)  | df[:3]grabs from the beginning up to bu not including the row at index 3 df.loc[]| purely label-location based indexer, for rows use index labels and for columns column name labels  | df.loc[0, ""fixed acidity""], df.loc[10:15, [""chlorides"", ""fixed acidity""]] df.iloc[]| purely integer-based indexer | df.iloc[0, 0], df.iloc[10:15, [0, 4]] df[""column_name""] < number | this just gives us a mask - tells us True or False whether each row fit's the condition, to use mask we need to use it to index into the dataframe using square brackets | df[""chlorides""] <= 0.08 and to use mask df[df[""chlorides""] <= 0.08]   || we can combine multiple requirements by using & | df[(df[""chlorides""] >= 0.04) & (df[""chlorides""] < 0.08)] df.query() | another way of grabbing desired data by specifying query parameters | df.query(""chlorides >= 0.04 and chlorides <= 0.08"") df.unique() | return the unique values  | df[""quality""].unique() df.groupby()| groups the values, does not return anything useful until we perform some aggregation on it | df.groupby(""quality"") df.count() | counts the data for each column | df.groupby(""quality"").count()[""fixed acidity""] groups the data by its quality and then counts it, the argument of count does not matter in this case as it is the same for every column df.sort_values() | sorts the values, default is ascending order | df.sort_values(""quality""), df.sort_values(""quality"", ascending = False) df.eval() | allows us to add columns to our dataframe (inframe is important as otherwise it isn't permanently added to our dataframe)| df.eval(""non_free_sulfur2 = total_sulfur_dioxide - free_sulfur_dioxide"", inplace = True) df.drop(""column_name"", axis, inplace) | allows you to drop columns/rows, axis=1 are columns and axis=0 are rows | df.drop(""non_free_sulfur2"", axis=1, inframe = True) df.fillna() | allows you to fill nulls, we can give it a default value to fill in or a number of other methods to fill it in (https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html) | df.fillna(-1, inplace=True) df.dropna() | allows you to drop nulls, drops the entire row of the missing value, include the axis | df.dropna(inplace=True) If you ask for a certain row/column of the data, it is returned as a 1-D array that has an index attached. This is an example of what Panda calls Series. For the most part you can treat them as a mini dataframe as they have a lot of the same methods. (https://pandas.pydata.org/pandas-docs/version/0.15.2/dsintro.html#series) Notebook 2: Pandas Exercises file: 2-Pandas-Exercise.ipynb This notebook contained practice exercises to use the commands learned in notebook 1 and 3. Notebook 3: Vizualization in Pandas file: 3-Vizualization.ipynb  usually other libraries such as Matplotlib and/or Seaborn are used for vizualizations, however Pandas has some plotting functionality build into it as Pandas plotting is  built on top of matplotlib we need to import it import matplotlib.pyplot as plt %matplotlib inline - this just tells the IPython notebook to plot it inline import matplotlib  matplotlib.style.use('ggplot') - changes the style that matplotlib uses (i.e. makes the plots look nicer than the default) plotting is available with Pandas via the plot()method, from there we can pass in a number of potential arguments to specify exactly how to build the plot most important argument is the kind keyword arguemnt, which tells the plot() method what kind of vizualization we would like barplot: kind = ""bar"" histogram: kind = ""hist"" scatter plot: kind = ""scatter"" boxplot: kind = ""box""  Example of plottting with Pandas Code  |  Explanation ------|--------- df.plot(kind=""scatter"", x=""free sulfur dioxide"", y=""total sulfur dioxide"") | scatter graph wih free sulfur dioxide on the x-axis and total sulfur dioxide on the y axis df.groupby(""quality"").mean()[""chlorides""].plot(kind=""bar"") | barplot of the mean amount of chloride gourped by quality"
23,day4-20201001.md,Shawn Vega,1601537894,"Day 4 - Thursday, 01 October 2020 Basics of Programming in Python continued Protocol by Chandra Today's Schedule  9 - 10: daily review of day 2 going through the protocol related to Basics of Programming in Python discussing and solution to open questions Quick tips to pull request approval by Tereza Tip : don't 'squash and merge' files from others    some tips on slack by Andrew   10 - 10:30: Introduction to the day's activities   Task for the day: Work on the new codealong notesbooks in the respository.  1st Repo: https://github.com/neuefische/2020-ds-Python2 2nd Repo: https://github.com/neuefische/2020-ds-Python3    10:30 - 12: Work through notebooks  Update progress in the google sheet: https://docs.google.com/spreadsheets/d/1wk4CyQJixCC2y3LVHSeMvMaixjeVeccHnWaRQOZ7HjQ/edit?usp=sharing  Use slack channel #ask-mia-and-tereza if stuck   12 - 13: lunch break  13 - 13:30: Check in with Mia and Tereza 13:30 - 16: Continue pair programming to work through the assigned tasks 16 - 16:30: Answer any outstanding questions and organisational topics Daily standup: What have I learned today? What will we do tomororow? Are there any blockers? No End of day questions (EOD) end of class  Today's Topics First Repo: Lists, sets and dictionaries in Python repo: https://github.com/neuefische/2020-ds-Python2 Notebook 1 Topics: Lists file: 1-Lists.ipynb  The codealong notebooks provide introduction and insight into the following topics of Python Introduction to Lists  lists are collections of ordered items List Operations (using append(), sort(), pop() etc) working with individuell elements in the List Example: accessing , removing, slicing using index  Ierating through the elements of the List  Notebook 2 Topics: Sets file: 2-Lists.ipynb The codealong notebooks provide introduction and insight into the following topics of Python Introduction to Sets  A set is defined as an unordered, mutable collection of unique items. Operation perfomed on Sets add an element, compare two sets, joining sets etc. Main difference between Sets and Lists, Sets holds only the unique values. Sets are faster than List in searching for an item present.  Notebook 3 Topics: Mutability file: 3-Mutability.ipynb The codealong notebooks provide introduction and insight into the following topics of Python Introduction to Mutability Mutability refers to the capability of an object to be changed after it has been instantiated  * List are Mutable unlike strings and tuples for example. Notebook 4 Topics: Dictionaries file: 4-Dictionaries.ipynb The codealong notebooks provide introduction and insight into the following topics of Python Introduction to Dictionaries  Dictionaries allow us to store a value associated with a keyword, they are unordered and Mutable. Adding new Key - value pair to the Dictionary Accessing values using keys and vice versa. Replacing Values  Dictionaries are iterables like Lists and Tuples  Second Repo: Functions, List and Dictionary Comprehension repo: https://github.com/neuefische/2020-ds-Python3 Notebook 1: Introduction to Functions file: 1-Functions_Exercise1.ipynb  The codealong notebooks provide introduction and insight into the following topics of Python Part 1  Reason for using functions is reusability. It stems from a methodology called Don't Repeat Yourself (DRY for short) Introductions to Built-in-functions that we already used like len(), range() etc. Function definition Syntax for defining a Function Calling a Function to desplay the result Use of Return statement  Notebook 2: Introduction to Functions continued file: 2-Functions_Exercise2.ipynb  The codealong notebooks provide introduction and insight into the following topics of Python Part 2  Usage of parameters and Arguments in a Function A parameter is the name of a variable given in a function definition An argument is the value that is passed to a function when it is called Working with Parameters with or without Default values  Notebook 3: Introduction to Functions continued file: 3-Functions_Exercise3.ipynb  The codealong notebooks provide introduction and insight into the following topics of Python Calling Functions with Positional Versus Keyword Arguments  Usage of positional and Keyword Arguments while calling a Function Definition of Local and Global variables  Notebook 4 Topics: Introduction List and Dictionary Comprehensions files: 4-List_Dict_Comprehension_Exercise4.ipynb  The codealong notebooks provide introduction and insight into the following topics of Python Comprehensions  Benifits of using Comprehensions Insights to different Comprehension technics List, Dictionary and Tuple comprehension, how to construct them  Pushing into Git repository How to push to github using the terminal:  cd to the folder from which you want to push git status changed files are marked in red git add type the first character of the file you want to select and press tab for autocompletion git status changed files are marked in green git commit opens vim, i to insert your commit message, esc to leave the message and :wq to save it                                / or git commit -m [your message] to avoid vim from opening git status “your branch is ahead of ‘origin/master’ by 1 commit"" git push done! "
24,day35_20201113.md,Rebecca Butler,1605281859,"Day 35 - Friday, 13th October 2020 Capstone Protocol by Martina  Schedule  9:00 - 9:10 Daily review 9:20 - 10:00 Questions to Mia 10:00 - 11:15 Information about the Capstone 11:15 - 12:00 Capstone preparation in teams 13:00 - 13:15 Questions about Capstone, go back in teams 14:00 - 16:00 Videos Neural Networks  Daily review & Questions to Mia  What is the most important information of notebook 8 we should get? make PCA to reduce the components (don´t consider unimportant components) clustering lowest BIC/AIC is best value to suggest a good number of clusters  Information about the Capstone  presentations eventually on Tuesday 07.01.2020, date will probably change  tool to work in teams: GitHub Projects (Kanban Board) define a goal  work together in a team on the same data sessions in Capstone Phase: Planning Session Weekly StandUp Stakeholder Review on Midterm open calls: Coaches are available for questions -> write questions in slack channel #ask-the-coaches topic related slack channels: EDA Supervised Machine Learning (not DL) Unsupervised Learning Image modeling NLP-Deep Learning Time Series Analysis Geo Data Analysis GitHub slack channel for code reviews: #code-reviews during the Capstone there will be optional lectures: SQL AWS GitHub Projects (Kanban Boards, Pull Requests, Issues) Time Series Analysis Geo Data Analysis Links for datasets: https://drive.google.com/drive/folders/1vL6JBny8n1uDILDEkxNMC-v4sXNceDlR?usp=sharing https://www.drivendata.org https://www.kaggle.com/competitions https://www.kaggle.com/c/cdp-unlocking-climate-solutions https://data.europa.eu/euodp/en/data/dataset/co2-emissions-data https://data.europa.eu/euodp/en/data/dataset/co2-emissions-data https://www.crowdai.org/challenges  Homework, Preparation for next week  watch videos and read about Neural Networks https://www.youtube.com/watch?v=bNb2fEVKeEo https://www.youtube.com/watch?v=yCC09vCHzF8 https://medium.com/oembot/a-sketchy-introduction-to-convolutional-neural-nets-68aee726fbd1 https://www.youtube.com/watch?v=d14TUNcbn1k "
25,day30_20201106.md,Shawn Vega,1604909469,"Day 30 - Friday, 6th November 2020 Data  Protocol by Chandra Schedule  09:00 - 10:00 Daily review 10:00 - 12:00 Retrospective activity   14:00 - 16:30 Data science life cycle   General question and answer   Github working together on same project   Better to work in different files For different topics use different branches avoid working on same jupyter notebook unless you know how to handle github well short demo by andrew about how to change branch from terminal   Git switch and git checkout performs same fucntion, git switch is just new. Link from Jenny for useful github commands   https://github.com/joshnh/Git-Commands    Tips about python script   Use functions for python script  Check the functions first before writing visual code  Keep the code clean    Retrospective activity   Four highest voted points discussed Links posted as help  Person who creates the protocol, updates the Doc file from andreas with links https://docs.google.com/document/d/1nQdWzBjb2akjyQie35lIliK0RZ1EjeHvvJB-Z6rpYus/edit?pli=1#      Help on debugging     * Google for the error message     * Work and test the codes in small chunks     * Try some Python IDEs for learning to debug     * Some useful links for debugging are below        https://code.visualstudio.com/docs/python/debugging        https://docs.python.org/3/library/pdb.html        https://towardsdatascience.com/ultimate-guide-to-python-debugging-854dea731e1b        https://towardsdatascience.com/must-use-built-in-tool-for-debugging-your-python-code-d5f69fecbdbe   Unclear description for project assignment     * Use pdf instead of google doc     * More clear expectations about project deliverables     * More feedback sessions about presenting technical part   Preparing presentaion on our own     * Better for less important topics, as we might missout on learning imortant topics     * Cheat sheet instead of presentaion might be better     * Below is the link to the whole activity       https://funretro.io/publicboard/CDcoUUGm1xT7SmMwLs57PazwrvR2/96998aab-bc66-4eb2-b199-a8020f544722 Data science life cycle   Data science life cycle overview       1. Business Understanding       2. Data Mining       3. Data Cleaning       4. Data Exploration       5. Feature Engineering       6. Predictive Modelling       7. Data Visualisation Miro board PDF is in the repository. More information on the life cycle please refer to link below    https://github.com/Jorrik1977/Data-Science-Lifecycle/blob/master/DS%20Lifecycle.ipynb Useful tips and info for presenting to your stakeholder   * Understand and present the business model   * Know your stakeholder   * Talk about target, business model and model performance to your stakeholder Clarification Variance bias analysis    * Always look how bad the error analysis while doing model evaluation    * Result of changing threshold, mia sent a link to graph how presicion and recall looks against threshold     https://www.saracus.com/wp-content/uploads/2018/09/precRecallVsThreshold.png     Don't forget the open questions  https://docs.google.com/document/d/1DdBqIdhwZoE8QxzyqsHqUsCD0cN9_zb03lBwnOzXcU0/edit?usp=sharing "
26,day32_20201110.md,Scott Small,1605043256,"Day 32 - Tuesday, 10 November 2020 Unsupervised learning - Dimensionality reduction & Principal component analysis (PCA) Protocol by Jenny Schedule  09:00 Discussing Jakolien's protocol about Day 31 10:00 Introduction to unsupervised learning by Tereza & Mia 11:15 Work on jupyter notebooks on unsupervised learning 12:00 Lunch break 13:00 Work on jupyter notebooks on unsupervised learning 14:45 Review & discussion of yesterdays' ""take-me-home challenges""  17:00 Daily standup  Discussing Jakolien's protocol about Day 31  Conda cheat sheet: https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf Google-drive folder containing the results of yesterdays' ""take-me-home challenges"": https://drive.google.com/drive/folders/1DWq5-EqriSPEsoGQFnLGpdGMwYAGh3pi?usp=sharing Google-docs sheet to share links of github repos for the projects: https://docs.google.com/spreadsheets/d/13nPA9zWuFaA4uCdvn-UA7-yPZZ-qpMdn4OgfIPw9x8Q/edit?usp=sharing  Introduction to unsupervised learning by Tereza & Mia Unsupervised learning - orientation What is unsupvervised learning?  Statistical methods that extract meaning from data without training a model on labeled data Models do not distinguish between target and predictor variables Purpose:  Find groups of data  Deal with the ""cold start problem"", when you do not have labeled data  Identify subpopulations that are not well represented in the overall population   Reduce the dimensions of the data Extension to EDA - when you have large number of variables.  Unsupervised learning domains   Dimensionality reduction - reducing the dimension of feature space by finding informative feature representations  Feature elicitation Structure discovery Meaningful compression Big data visualization in 2D    Clustering - identification of meaningful groups in the data in the absence of labeled data  Recommender systems Targetted marketing Customer segmentation    Dimensionality reduction The curse of Dimensionality   If we divide a region of a space into regular cells, the number of cells grows exponentially with the dimensionality of the space.      ➔ We would need exponentially large quantity of training data to ensure all cells are filled.    We can still find pattern recognition techniques applicable to high-dimensionality data by exploiting the properties of real data:  Real data... may be confined to a region of the space having lower dimensionality. Therefore, the directions over which important features vary can be confined.  will typically exhibit some smoothness properties and small changes in the input variables will produce small changes in the target variable.            ➔ use local-interpolation techniques to make predictions for new values.  Advantages and disadvantages of dimensionality reduction  Advantages Speeds up subsequent algorithm Data compression without substantial loss of information Helps visualizing patterns  May improve results through noise reduction (only sometimes)   Disadvantages  Potential information loss Computational cost Transformed features may be hard to interpret  Dimensionality reduction using Principal component analysis (PCA) Assumptions of PCA  Often variables vary together. Some of the variation of one variable is duplicated by the variation of another.      ➔ identify how variables covary.  General idea of PCA  Combine the numeric predictors into a smaller set of variables, which are weighted linear combinations of the original set:  The smaller set of variables (= the principal components), explains most of the variability of the full set of variables. The weights reflect the relative contributions of the original variables. Manifold Hypothesis  “Most real-world high-dimensional datasets lie close to a much lower-dimensional manifold”  Getting the best point of view = maximizing the line of sight Examples Typical example: swiss roll  = 2D plane, bent and twisted in 3rd dimension  the idea is to unroll the swiss roll to find the ""real distance"" between two points (which is different when roll is rolled together)    One dimension example: line, circle, but not 8 Two dimensions example: surface, sphere, plane    How does PCA work?  Goal: reduce dimensions of feature space, while preserving as much information as possible Transform data for PCA - centered around zero. Find new axis (= principal components) that represent the largest part of the variance by projecting to lower dimensions  Principal components (PC) are combinations of features and can be presented in the original feature space by projection.  Maximize the variance, while observing the following restrictions:  PC must be orthogonal (=independent) to each other. You can have just as many PC as features.      Keep only the most informative PC.   After identifying all PC, reduce the dimensionality of the dataset by keeping only the first d PC. Look at the explained variance ratio of the PCs to decide how many d Dimensions to keep. Take as many d PC that a sufficiently large portion of the variance is explained (eg 95%).      To make variance-calculations more easy, principal components are found by a standard matrix factorization technique (Singular Value Decomposition (SVD))  Great visualization of PCA procedure: https://setosa.io/ev/principal-component-analysis/  Variants of PCA  Randomized PCA: quick, approximation of first d components Incremental PCA: for parallelization works with minibatches Kernel PCA: Kernel trick Complex nonlinear projections are possible Preserves clusters of data after projection Can help to unroll data that lies on a manifold    Further techniques  Locally Linear Embedding (LLE) Nonlinear dimensionality reduction technique Preserves local relationship Able to unroll twisted manifolds Linear Discriminant Analysis (LDA) Classification algorithm Finds discriminative axes that keep classes as far apart as possible  t-Distributed Stochastic Neighbour Embedding (t-SNE) Mostly used for visualization  Summary on PCA  Advantages Unsupervised learning - no labelling of data required Reduces dimensionality Simpler representation of variable correlation Critical assumptions/limitations Assumes linear relationship among explanatory variables  Target variable is the variance.. tricky with noise Disadvantages Works only on numeric features  No missing values support  Review & discussion of yesterdays ""take-me-home challenges"" Feedback:  Put a summary of conclusions at the beginning of the jupyter notebook Include a summary of conclusion in your README.md Clearly state the assumptions and limitations of the EDA and the model Explain why you do which steps in the analysis Present content in ""digestable"" and easy to read manner Formatting:  Make titles large for better orientation in the notebook Use markdown or print-out to make conclusions/explanations stand out from code Make markdown descriptions directly under the plot/calculation to prevent zig-zack-reading and searching Where possible combine code in one larger cell, instead of having many small cells with code Delete out-commented and double code Number formatting: round it to 2 or 3 decimals for better readability (Code: pd.options.display.float_format = ""{:.2f}"".format) Use the ""spelling check plugin"" in your jupyter notebook Write down according to which scores your model is good or bad Let your domain knowledge shine through Use the company colors to impress  Miscellaneous  Topic of tomorrow: unsupervised learning domain 2. - Clustering "
27,day12_20201013.md,Tamara Thompson,1602621336,"Day 12 - Tuesday, 13th October 2020 More on Regression Analysis & Introduction to EDA project Protocol by Sina Schedule  9.00 - 10.00: Daily review of day 11  Q&A on Linear Regression Notebooks 1-6     * Why is adding a constant necesssary ( X = sms.add_constant(X))?     Constant is starting point for calculating the intercept; the intercept has to be multiplied by something      For predicting values with your fitted model use ```name_of_your_model.predict([1, x_value])```; the _1_ is the constant you multiply your intercept with     * How to decide which model to use?      Decision should be based on R_adj  * endog, exog, what is that?      Are used within statsmodels, f.e. for creating a OLS models (```sms.OLS(endog, exog)```); for *endog* place the dependent variable, for *exog* the independent variable(s)   09.45 - 10.30: Work on Notebook 7  Categorical variables   10.30 - 11.15: Introduction to EDA project    11.15 - 12.00: Work on EDA project   12.00 - 13.00: Lunch break    13.00 - 15.30: Work on EDA project   15.30 - 16.00: Daily check out   Work on Notebook 7 Notebook can be found at https://github.com/neuefische/2020-ds-Linear-Regression.git What are categorical variables  categorical variables represent categories instead of numerical features  Identifying categorical variables  combine .describe()/.info() with plotting the variable in a scatterplot or histogramm  Transforming categorical variables Approach 1: Label encoding Command | Explanation -----|------- df[""column_name""] = df[""column_name""].astype(""category"") | Converts dtype of Pandas Series into category df[""column_name""].cat.codes| Assigns numeric values to categories OR: Use scikit-learn's LabelEncoder (no need to transform dtype of variable) Command | -----| from sklearn.preprocessing import LabelEncoder|  lb_make = LabelEncoder()| column_name_encoded = lb_make.fit_transform(df[""column_name""])| Approach 2: Create dummy variables The idea is to convert each category into a new column, and assign a 1 or 0 to the column pd.get_dummies(df[""column_name""]) OR: Use LabelBinarizer in scikit-learn Command | -----| from sklearn.preprocessing import LabelBinarizer|  lb = LabelBinarizer()| origin_dummies = lb.fit_transform(df[""column_name""])| origin_dum_df = pd.DataFrame(origin_dummies,columns=lb.classes_)|  The dummy variable trap Multicollinearity: Perfect Multicollinearity states that one variable can perfectly be predicted from the other variables This is a problem for regression analysis! Thus, drop one of the dummy variables: pd.get_dummies(df[""column_name""], drop_first = True) Dropped category becomes reference category: Interpretation of coefficients of remaining variables always in relation to this category! (f.e. being from origin 2 is associated with an average increase/decrease of ? mpg in comparison to origin 1) Introduction to EDA project The task can be found here: https://docs.google.com/document/d/1tQBNzrWkyoxdI6ijDS613U1hN_5eBcy3Z9Zairro8HY/edit?usp=sharing The code and the column description can be found here: https://github.com/neuefische/2020-ds-Project-EDA.git EDA checklist: https://d6-neuefischedatahh.slack.com/files/U01A2B0NDSB/F01CGBUDCJW/eda_checklist.pdf General information:  Solve assignment, push Jupyter Notebook, python script, README file and a presentation to own github repo Latest upload: 15.10.2020 11:00 Duration of presentation should be 10 minutes There will be a Q&A-session on Wednesday (14.10.), where you can ask your questions regarding the dataset: 5 minutes for Q and 5 minutes for A per person Jupyter Notebook should contain explanations, thought processes and only relevant plots and analyses, which contribute to the confirmation of own assumptions  More about the task First part of the task  Creating a new environment conda create --name kingcounty python=3.8.5 conda install -n kingcounty pytest==6.1.1 conda install -n kingcounty ipython conda install -n kingcounty jupyterlab conda install -n kingcounty seaborn conda install -n kingcounty scikit-learn EDA on the dataset with the help of the checklist  Second part of the task Model dataset with a multivariate linear regression to predict the sale price of houses : * Split dataset into train and test set  (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)  train a simple model fit model on trained data use fitted model on test data "
28,day9_20201008.md,Rebecca Butler,1602179427,"Day 9 - Thursday, 8th October 2020 More to Pandas and introduction to Numpy Protocol by Martina Today's Schedule  9 - 10: daily review of day 8 going through the protocol ""Introduction to Pandas""  discussing and answering open questions  What are the file extensions good for? Could Jupyter Notebook read every filenames? How do I change a pull request? Do I have to make the changes on my machine first?  You can make the changes on GitHub but it´s better doing them on your own machine. Pull request will be updated with the last changes. How do I make a pull request of only one file?   Can be deleted or only the changes will be updated.    10 - 10:30: Introduction to the day's activities  Tereza showed how to create a new test file (related to the Polish-Reverse-Calculator) Task for the day: work through the condelong notebooks in the repository 2020-ds-Pandas-Numpy (https://github.com/neuefische/2020-ds-Pandas-Numpy) 12 - 13: lunch break 13 - 13:30: Check in with Mia and Tereza 13:30 - 15:30: Continue pair programming to work through the assigned tasks 15:30 - 16:00: Answer any outstanding questions and organisational topics Information: If you want to unpack something, use '' (e.g. print(iris.sepal_length[:3].mode()) Daily standup: What have I learned today? What will we do tomororow? Are there any blockers? No End of day questions (EOD) 16:00 - open end: Continue pair programming to work through the assigned tasks end of class  Repo Pandas and Numpy Notebook 1: Pandas file: 1-Pandas.ipynb  Pandas is good at dealing with data that is: Tabular/Column data (e.g. csv files) Time series Medium data < 1 million rows < 5GB of data   Pandas is not good at dealing with data that is: Text / Image / Audio Big data sets 1 million rows 10GB start with a clean namespace. It is a good habit to get into: reset -fs to get access to everything in the pandas library you need to import it: import pandas as pd, import numpy as np pd is the standard way to import Pandas library np is the standard way to import Numpy library  Command | Explanation | Example  -----|------- |------- A = [name].T | Transpose data from pandas.plotting import scatter_matrix scatter_matrix([name][[selected colums]], diagonal='hist' | draws a matrix of scatter plots | scatter_matrix(df_abalone[[""length"", ""diameter"", ""height""]], diagonal='hist'); Summary  Pandas' DataFrame is the de facto data structure for Data Scientists Get your data into a DataFrames ASAP Use built-in methods to go fast! For example, use groupby to calculate aggregate statistics for categories  Notebook 2: Pandas Practice 1 file: 2-Exploratory_Data_Analysis(EDA)_Exercise_1.ipynb This notebook contained practice exercises to use the commands learned in the notebooks before. Notebook 3: Pandas Practice 2 file: 3-Pandas_Exercise2.ipynb This notebook contained practice exercises to use the commands learned in the notebooks before. Command | Explanation | Example  -----|------- |------- pd.crosstab() | computes a simple cross tabulation of two (or more) factors. By default computes a frequency table of the factors unless an array of values and an aggregation function are passed. Notebook 4: Intro to Numpy file: 4-Numpy_Pandas.ipynb  to get access to everything in the Numpy library you need to import it: import numpy as np np is the standard way to import Numpy library  Making a numpy array Command | Explanation | Example  -----|------- |------- np.array([input]) | creates a numpy array | my_lst_ndarray = np.array([1, 2, 3, 4, 5]) np.array((input, np.int32) | specifies the data type | np.array((1, 2, 3, 4, 5), np.int32)  np.array([input]) | creates a numpy array | my_lst_ndarray = np.array([1, 2, 3, 4, 5]) np.shape | gives us the number of rows and number of columns np.dtype | tells us the data type of the objects in our ndarray np.zeros((a, b)) | creates a matrix of zeros with 'a' rows and 'b' columns | zeros_arr = np.zeros((3,4)) np.ones((a, b)) | creates a matrix of ones with 'a' rows and 'b' columns | ones_arr = np.ones((10,20)) np.identity(a) | creates an identity matrix with 'a' rows and 'b' columns | identity_arr = np.identity(50) np.random.rand(a, a) | creates a axa array of random floats ranging from 0 to 1 | random_arr = np.random.rand(2, 2) np.arange(a, b, c) | creates a numpy array with arguments (start, end, step_size) | range_arr = np.arange(0, 20, 0.5) Numpy array math  basic mathematics operators available: +, -, , /, *, and % two arrays that are the same size, in which case these operators will just be performed elementwise  Command | Explanation  -----|------- | [array1] - [a, b] | subtracts 'a' off the first column and 'b' off the second column [array1] - [[a], [b]] |  subtracts 'a' from the first row and 'b' from the second row More about numpy Command | Explanation | Example -----|------- |---- Indexing range_arr[:, a] | grabs every row, but only the element at index 'a' in those rows range_arr[a:b] | with no second index, this defaults to taking the rows range_arr[a:b, c:d] | the first set of numbers refers to the rows to grab, the second set the columns Other methods range_arr.sum(axis=0) | sums along the rows (i.e. get column totals) range_arr.sum(axis=1) | sums along the columns (i.e. get row totals) range_arr.sum() | gets sum of all elements in numpy array range_arr.mean(axis=0/1) | grabs the mean along the rows/columns range_arr.std(axis=0/1) | grabs the standard deviation along the rows/columns range_arr.max(axis=0/1) | grabs the maximum along the rows/columns range_arr.min(axis=0/1) | grabs the minimum along the rows/columns range_arr.argmax(axis=0/1) | maxes of each column occur at row/ row occur at column 1 (index 0) range_arr.argmin(axis=0/1) | mins of each column occur at row/ row occur at column 1 (index 0) range_arr.argmax() | get the index of the overall maximum (the last index) range_arr.argmin() | get the index of the overall minimum (the 0th index) range_arr.cumsum(axis=0) | gets the cumsum along the rows (i.e. from top to bottom) range_arr.cumprod(axis=0) | gets the cumprod along the rows range_arr.flatten() | return a copy of the array collapsed into one dimension range_arr.ravel() | returns a contiguous flattened array np.where([condition]) | returns the indices where the data meet the condition | np.where(my_ndarray <= 2), np.where(my_ndarray == 8) Combining Datasets with Pandas Command | Explanation | Example -----|------- |---- df.join() | joins dataframe back to another dataframe, you have to rename the columns | joined_df = wine_df.join(quality_dummies) pd.concat() | concatenate pandas objects along a particular axis | joined_df2 = pd.concat([quality_dummies, wine_df], axis=1) pd.merge() |  | pd.merge(red_wines_quality_df, white_wines_quality_df, on=['quality'], suffixes=[' red', ' white']) Pivot Tables Among other functions, a pivot table can automatically sort, count total, or give the average of the data stored in one table or spreadsheet, displaying the results in a second table showing the summarized data. Pivot tables are also useful for quickly creating unweighted cross tabulations. Command | Explanation | Example -----|------- |---- pd.cut() | turns a column with continuous data into categoricals by specifying bins to place them in | pd.cut(red_wines_df['fixed acidity'], bins=np.arange(4, 17)) pd.pivot_table() | creats a pivot table | pd.pivot_table(red_wines_df, values='residual sugar', index='quality', columns='fa_bin') As you might have guessed, we have functionality to create pivot tables available for our use in Pandas. The way that we do this is by calling the pivot_table() function that is available on the pandas module (which we've stored as pd). As the docs tell us, the pivot_table() expects a number of different arguments:  * data: A DataFrame object  * values: a column or a list of columns to aggregate  * index: a column, Grouper, array which has the same length as data, or list of them. Keys to group by on the pivot table index. If an array is passed, it is being used as the same manner as column values.  * columns: a column, Grouper, array which has the same length as data, or list of them. Keys to group by on the pivot table column. If an array is passed, it is being used as the same manner as column values.  * aggfunc: function to use for aggregation, defaulting to numpy.mean"
29,day26_02112020.md,Kristina Norris,1604388292,"Day 26 - Monday, 02 November 2020  Protocol by Sebastian Schedule   9:00 - 09:15 Daily review of day 25   09:30 - 12:00: Work on second project   12:00 - 13:00: Lunch Break   13:00 - 13:30: Q&A    13:00 - 16:00: Team work second projekt    16:00 - 16:30: Q & A   16:30 - 18:00: Work on project again   Daily review: questions/ discussions  How is the progress on the notebooks Did any problems occur? Short review so we can work on our projects again  Q & A  Does everybody have his 1st milestone completed? Is the business case set? Are you happy with the project so far? Frustration because of progress, most people have no simple model so far, still in the generall process of dealing with the data New challenges regarding standardization or normalization of data, merging datasets, low interpretability Visualizations are the biggest time consumers   M & T' s guidance: Do not get hung up on the details (nice visualizations) Focus on your business case objective -> easy assumptions should be answered by your model Save enough time for preparing your presentation slides, so you can adjust your analysis, visualisations and outcomes    to possible new occuring interesting features   "
30,day23_20201028.md,Sara Dawson,1603916572,"Day 23 - Wednesday, 28th October 2020 Protocol by Michael 4 Ensemble Methods  Schedule  09:00 - 09:30: Daily review 09:30 - 12:15: Introduction into Ensemble Methods 12:15 - 13:15: Lunch break 13:15 - 16:00: Working on notebooks 1-3 16:00 - 16:30: Daily check out 16:30 - 17:30: Continue working on notebooks 1-3   THE END  Q & A // Tips & Tricks Handling of outliers & missing values  The handling of outliers is dependent on the purpose/aim - What do we want to do with the data? What does the model work with? Set rules - What are outliers? If there are 'too many' outliers the data might have changed Apply clean function to test & train data set Document outliers Handling missing values - Options: Exclude it Replace/Fill it    https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b https://cxl.com/blog/outliers/   Hyperparameter in Support Vector Machines  C Hyperparameter to control the error Low C --> less error Large C --> high error There is no rule of thumb that low C will always work or high C or medium C. Low error doesn't mean that we have a good model (the error is dependent on the data set) Gamma Only for the Gaussian RBF kernel Decides how much curvature we want in a decision boundary Gamma is high --> more curvature Gamma is low --> less curvature   https://medium.com/@myselfaman12345/c-and-gamma-in-svm-e6cee48626be  Random Forests w/ or w/o Bootstrap  Bootstrap - Tells if samples of observations should be drawn with or without replacement Bootstrap = True - draws a sample of size equal to the number of training examples with replacement Bootstrap = False - draws a sample of size equal to the number of training examples without replacement The data is randomized for both conditions of bootstrap https://stackoverflow.com/questions/40131893/random-forest-with-bootstrap-false-in-scikit-learn-python  https://stats.stackexchange.com/questions/354336/what-happens-when-bootstrapping-isnt-used-in-sklearn-randomforestclassifier  Variance  Reduce variance caused by overfitting Limiting max depth and/or number of leaf nods Use several Trees ==> Random Forests  Tips & Tricks  Seperate X and y using .pop()-Method The Laura-Way-of-Working Hear Do & Apply Go through theory again Use Jupyter Nb instead of Jupyter Lab for 3D graphs.  Introduction into 4 Ensemble Methods 4 Additional supervised learning algorithms. Summary: https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/ 4 Types of Ensembles  Bagging Using different Models Boosting Stacking  Recap on Decision Trees  CART is s synonym for Decision Trees CARTs are greedy --> In order to find the best nods, CART looks at all nods (try's every combination), which is 'expensive' CARTs tend to overfitting --> To avoid overfitting trees can be pruned (define max leafs, max depth, max sample) CARTs unstable --> Small changes in the input can lead to large changes in the tree structure  Because Trees are greedy and overfitting rather ask your friends, than try everything your own... (Friends analogy) Ensemble Learning Ask for more opinions before making a judgement call... https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/ Ensemble learning = A model that makes predictions based on a number of different models  Several models/ algorithms work together on the same data set Decreases error Less sensitive to overfitting Mechanism --> Majority voting Ways of Majority voting: Bagging: same model on different parts of the data Different models same data Boosting: seq. adjusts for importance of observations Stacking: training in parallel and combining Needed is one prediction out of several predictions --> Aggregation by mean, median, mode, rank ... How do we vote? Hard voting --> Taking the majority class (like SVM's; no probabilities used) Soft voting --> Take average of probabilities Why does it work? Not all predictions can be wrong at the same time Prerequisites to work Models better than random --> each weak learner contributes a little by being a little right than a little wrong (49% wrong vs 51% right) Independent / Non correlated models Sufficient amount of weak learners weak learner = A model that does slightly better than random predictions is a weak learner Check for independence calculating the pearson coefficient for predictions (r --> 0)  1 - Bootstrap Aggregating - Bagging  https://en.wikipedia.org/wiki/Bootstrap_aggregating https://www.machinelearningplus.com/machine-learning/gradient-boosting/  Statistical method of resampling --> Repeatedly calculating statistics based on the same sample Bagging = Training several models in parallel. Each model learns from a random subset of the data, where the dataset is same size as original but is randomly sampled with replacement (bootstrapped). Example: Given a standard training set D of size n. Bagging generates m new training sets Di (each of size n') by sampling from D uniformly and with replacement. Training M different trees (stumps) on different subsets of data chosen randomly with replacement Bagging also used for A/B-Testing To avoid highly correlated predictors because of running the same algorithm in different subsets --> Random Forests  Random Forests (a application of bagging)  Random Forest = parallel combination of decision trees. Each tree is trained on random subset of the same data and the results from all trees are averaged to find the classification.  Random Forests  https://stats.stackexchange.com/questions/175523/difference-between-random-forest-and-extremely-randomized-trees https://quantdare.com/what-is-the-difference-between-extra-trees-and-random-forest/ A Random Forest consists out of several decision trees/ stumps Decorrelate predictors by (Randomly) Chosen subset of input variables (Randomly) Chosen subsets of data Pros Gain of performance Gain in accuracy (compared to normal decision trees) Decreasing variance Cons Loss of interpretability Extremely randomized trees --> Introduce variation by changing the way we built trees (use different stumps)  2 - Different Models  Because mixing models which are very different in their nature, you get better  results. Models to mix: Decision Trees, Random Forests, Neural Networks, SVMs, KNN ...  3 - Boosting  https://www.machinelearningplus.com/machine-learning/gradient-boosting/ Boosting = Training several models sequentially. Each model learns from the mistakes of the previous model. That is, the subsequent models tries to explain and predict the error left over by the previous model. Why Boosting? - Boosting works on the principle of improving mistakes of the previous learner through the next learner. Boosting is a greedy algorithm for fitting adaptive basis function algorithms Way of working - In boosting, weak learners (ex: decision trees with only the stump/ only one fork) are used which perform only slightly better than a random chance. Boosting focuses on sequentially adding up these weak learners and filtering out the observations that a learner gets correct at every step. Basically, the stress is on developing new weak learners to handle the remaining difficult observations at each step. Boosting can be seen as 'gradient descent in the function space' Different loss-functions possible --> Applying different loss functions =  Different boosting algorithms (CART is commonly used) Boosting-Methods: L2-Boosting Ada-Boost Logit-Boost Gradient-Boosting   Hyperparameter Learning rate = Learning rate, denoted as α, controls how fast the model learns. This is done by multiplying the error in previous model with the learning rate and then use that in the subsequent trees. So, the lower the learning rate, the slower the model learns. (slower = more accurate) N_estimatorsis = Number of trees used in the model. If the learning rate is low, we need more trees to train the model. However, we need to be very careful at selecting the number of trees. It creates a high risk of overfitting to use too many trees. Picked by observing the performance/error --> Stops if performance/erros starts to decrease/increase (early stopping) Returns the number of trees needed Forward stagewise additive modeling --> Does not go back to adjust earlier parameter (e.g. New decision generated in each iteration --> # iterations = # models) Pros Resistant to overfitting  Bagging vs. Boosting  4 - Stacking  Train weak learners (models) in parallel Chose model that fits the best for your observation Works well if test data = train data Cons Ted to overfitting --> cross validation  Which Model Should We Use?  Common approach - Try different ones (take the one which works the best for your domain) For low dimensional features: Boosted Decision Trees Random Forests Bagging Decision Trees  Interpretation of Models (BlackBox)  Linear Models Good interpretability Poor performance Evaluate the effect of variables in complex models Partial Dependence Plot Feature Importance  Repo - 2020-ds-Ensemble-Methods-1 Notebook 1: Codealong on Random Forests file: 1-Random_Forest_Codealong.ipynb Notebook 2: Tutorial on Random Forests file: 2-Random_Forest_Tutorial.ipynb Notebook 3: Exercise on Random Forests file: 3-RandomForest_Lending_Club_Exercise.ipynb Notebook 4: file: 4-Ensemble_Methods_Part1.ipynb --> Task for October 29th"
31,day20_20201023.md,Tammy Sexton,1603459493,"Day 20 - Friday, 20th October 2020 Decision Trees Protocol by Laura Schedule  9:00 - 9:30 Daily review Looked at different ways of writing the KNN function 10:00 - 11:00 Lecture on Decision Trees 11:00 - 16:00 Work on notebooks 16:00 - 16:15 Daily Stand up and Beer Have been provided with slides on classes to have a look at   Lecture What is it about?  Machine learning algorithm based on binary trees (parent nodes have at most 2 children nodes) Decision trees learn the relationships between observations in training sets Each leaf is responsible for making a decision Regression trees: prediction is a value (price of socks) Classifier trees: prediction is a target category (sock.. Not sock) Mainly used for exploring data and not for predicting due to overfitting Easy to interpret (white box) No feature scaling or centering needed No assumptions needed (linearity, eg) Building block of Random Forests, Ada boost, Gradient boost Training is costly, prediction is cheap  Desicion Tree in words: * Each path from root of the tree to leaf predictors passes by a series of internal decision nodes * Each decision node compares a feature to a split point * Node’s feature and split point are chosen during training * This is done recursively until a stop condition is reached.. (e.g. less than 5 observations in a node) Decision Trees - Visualized (pip install dtreeviz) https://github.com/parrt/dtreeviz/blob/master/testing/gen_samples.py  Decision tree eyecandy: http://www.r2d3.us/visual-intro-to-machine-learning-part-1/ Interpreting Decision Trees - visualization factors  Decision node - Feature vs target value distribution - how separable are target values based on the feature and split value Decision node - Feature name and split value Leaf node purity - leaf nodes with low variance or overwhelming majority class are more reliable predictors Leaf node prediction value Number of samples in decision node Number of samples in leaf node: too few might be a sign of overfitting Following a specific feature down the tree  What is the idea behind decision trees  Decision trees are build on questions Stump: root node with just one split Understanding the boxes: samples: number of values in that leaf value : [number in category 1, number in category 2]  Impurities * Both leaves are impure, ie do not divide the instances into pure groups * In order to built a tree, we measured how pure the data is split e.g. gini, entropy (measure of disorder) * Gini impurity: can be computed by summing the probability of an item with label i being chosen times the probability of a mistake in categorizing that item      * For decision treest the gini impurity can be be between 0 and 0.5 -> when close to 0 the data is unequally split while if gini is 1 the data is equally split between the two categories Applying Gini  Gini impurity before any split has happened is just based on the overall split The Gini impurity after the split is the weighted average of the impurities of the leaves: multiple the percentage of sample per leaf by the gini of that leaf, then add it together for the two leaves Gini Gain = (Gini Impurity of Root) - (Weighted Gini impurity of Leaves) Calculate the Gini Gain of alternative features: Choose the feature with the highest Gain for the split Next level of leaves: Apply the same steps From top to bottom the features giving the highest Gini gain are used to split  Ways to limit overfitting: limit the number of branches/leaves  max_leaves  max_depth  min_sample_split  max_features   If the feature is continuous, trees work just as fine  The data are ordered in ascending order For each mean between to adjacent data points the impurity is calculated The split is made where impurity is lowest  Regression: Continuous target variable  Same approach as with classification tasks Now: Split the data based on Mean Squared Error (MSE) rather than impurity The prediction value is the mean of all observed values falling in the respective range Traverse the tree to get the right one Regularization is needed  Notebooks Repo: https://github.com/neuefische/2020-ds-Decision-Tree 1. Decision Trees Classification 2. Decision Trees Regression 3. Decision Trees Recursion (Blog-Post)"
32,day28_20201104.md,Sarah Pace,1604506626,"Day 28 - Wednesday, 04 November 2020 There is no terror in the bang, only in the anticipation of it.  Protocol by Andreas Schedule  9:00 -  9:10 Daily review of day 26 09:10 - 12:00 Work on second project 12:00 - 13:00 Lunch Break 13:00 - 16:00 Work on second project 16:00 - 16:20 Status report 16:20 - 16:30 Q&A 16:30 - Open end Work on second project (or not)  Q&A How to do error analysis https://towardsdatascience.com/error-analysis-to-your-rescue-773b401380ef What is more common in industrie: Regression or Classification?  Depends on business domain At FREE NOW it was Regression and Forecasting  How to fix problems with environment or python-version in VSCode's terminal?  Set the latest python version as default: Go to Code > Preferences > Settings There search for ""terminal"" and click on ""Edit in setting.json"" under ""Terminal › Integrated › Env: Osx"" There add the following text in new lines:    ""terminal.integrated.env.osx"":{                                   ""PATH"": """"                                   } Save the changes and restart VSCode    Delete a wrong envoironment: Just type in your terminal (ENV_NAME should be the environment you want to remove, e.g. nf_simple)    conda env remove -n ENV_NAME    Export your environment to a .yml-file: Type the following while the chosen environment is active    conda env export > environment.yml "
33,day5_20201002.md,Kristina Lin,1601746731,"Day 5 - Friday, 2 October 2020 Group presentations on a Unix tutorial & Introduction to AI Protocol by Jakolien Today's schedule  9:00 - 10:00      Daily review of day 4 read through the protocol of day 4 written by Chandra learned: how to push to github using the terminal (see protocol day 4) to highlight text in grey in markdown, the text needs to be preceded and followed by three backticks   helpful URL on how to write in markdown: https://guides.github.com/features/mastering-markdown/ 10:15 - 12:00     Collaborating on group presentations on using the terminal read through the Unix tutorials The tutorials can be found here: http://www.ee.surrey.ac.uk/Teaching/Unix/index.html   prepared presentations in groups The presentations can be found here: https://github.com/neuefische/hh-2020-ds4-daily-review/tree/master/unix_tutorial_presentations   12:00 - 13:00 Lunch break 13:00 - 14:00 Presentations 14:15 - 15:30 Class on Introduction to AI The slides can be found here: https://d6-neuefischedatahh.slack.com/files/U01A2B0NDSB/F01BRMCMC06/3_machine_learning_intro.pdf?origin_team=T019HBUU0QP&origin_channel=C01AAP86WU9 15:30 - 15:45 Daily standup 15:45 - 17:15 Drinks  General on presentations  Jupyter Notebook can be used for presenting markdown and code. slidesgo.com is a website with fancy templates for presentations.  Cheat sheet based on Unix tutorials A command, e.g. ls, can be followed by arguments, e.g. -la and ~/directory/folder/file. An option is a documented type of argument, e.g. -la, which lists the visible and hidden files in a directory. All commands are executed on files or folders in the working directory, unless a path is specified. Tutorial 1 - Files and directories by Kevin, Sina and Jakolien Command | Arguments | Meaning --------|-----------|-------- cd | [dirname] | change to specified directory cd | ~ | change to home directory cd | .. | change to parent directory ls | | list visible files ls | -la | list visible and hidden files ls | ~ | list visible files in home directory mkdir | [dirname] | make a directory with specified name pwd | | print working directory rmdir | [dirname] | remove specified directory only works for empty directories rmdir | -r [dirname] | remove specified directory regardless whether it is empty or not Tutorial 2 - Files and their contents by Andrew, Laura, Petra and Sebastian Command | Arguments | Meaning --------|-----------|-------- cat | [filename] | print contents of specified file use less for long files cp | [source_filename] [target_filename] | copy and paste specified file grep | [keyword(s)] [filename] | globally search for a regular expression and print it: search for a specified keyword in the specified file and print the line with the keyword grep | -i [keyword(s)] [filename] | case sensitive search of specified keyword in specified file grep | -ivc [keyword(s)] [filename] | print lines without the specified keyword in the specified file head | -[d] [filename] | print the first [d] lines of specified file less | [filename] | print first page of specified file use the space bar to move to the next page less | [filename]/[keyword] | highlights specified keywords on the page of the file with the specified filename mv | [source_filename] [path/target_filename] | move specified file to specified folder use the same path with another target_filename to rename the file rm | [filename] | remove specified file tail | -[d] [filename] | print the last [d] lines of specified file wc | -c [filename] | character count of specified file wc | -l [filename] | line count of specified file wc | -w [filename] | word count of specified file Tutorial 3 - Redirection and pipes by Chandra, Martina and Ole Command | Arguments | Meaning --------|-----------|-------- cat | > [filename] | stores written content in a new specified file [ctrl+d]to end input cat | >> [filename] | appends written content to specified file [ctrl+d]to end input cat | [filename1] [filename2] > [filename3] | concatenates files into a new specified file sort | < [filename] | sorts the lines in the specified file and prints the specified file Pipes '|' are used to apply all commands at once, e.g. cat [filename1] [filename2] | grep p | sort prints all lines containing a p in the two specified files and sorts the lines. Tutorial 4 - Wildcards, filenames and help by Andreas, Jenny, Michael and Volodymyr Command | Arguments | Meaning --------|-----------|-------- ? | | one character * | | any number of characters ls | [word] | list all files starting with [word] ls | [word] | list all files ending with [word] ls | [w]*[d] | list all files starting with [w] and ending with [d] Filenames need to be comprehensible and should not contain special characters, such as /, *, %, &, [space] and [tab]. Command | Arguments | Meaning --------|-----------|-------- apropos | [keyword] | provides manual page for command with [keyword] in header man | [command] | manual page for [command] including options whatis | [command] | info excluding options Tutorial 5 - Permissions and processes by Andreas, Jenny, Michael and Volodymyr Permissions Long listing (ls -l or ls -lg) displays a 9 letter string with permissions of the owner, the group and all users as well as the owner, the directory, the file size, the date and time of creation and the filename Command | Arguments | Meaning --------|-----------|-------- ls | -l | long listing ls | -lg | long listing with group info ls | -lag | list access rights for all files chmod | [a/g/o/u] [+/-] [rwx] [filename/dirname] | change permissions on files and directories Who | | a | all | | g | group | | o | other | | u | user Add/Remove permission | | + | add permission | | - | remove permission Permissions on files | | r | read files | | w | write files | | x | execute files Permissions on directories | | r | list files in the directory | | w | delete files from directory or move files into it | | x | access files in the directory Processes Command | Arguments | Meaning --------|-----------|-------- [ctrl] z bg | | background suspended job [command] | & | run command in background fg | %[d] | foreground job number [d] jobs | | list current jobs kill | %[d] | kill job number [d] kill | [PID] | kill process with Process ID [PID] kill | -9 [PID] | enforce kill process with Process ID [PID] ps | | list current processes PID is unique process ID [ctrl] c | | kill job running in foreground [ctrl] z | | suspend job running in background Introduction to AI  AI is a branch of computer science. AI helps people doing what they can already do, but with more complicated mathematical models and with more data. Narrow AI can solve one task only, like playing chess. The terms machine learning and neural networks already exist since 1959, respectively 1958, but gained popularity now that computing power is at scale.  Using AI can be dangerous, and therefore you always need to check thoroughly what your algorithm does and whether it can be used against mankind.   Machine learning can be used to determine:  quantity class group something weird  action   There are three types of machine learning: supervised, unsupervised and reinforcement machine learning. This is shown in Figure 1.    Figure 1: three types of machine learning.  1. Supervised machine learning: used when you know your training data e.g. when computing housing prices based on the location and the number of square meters of the house.   common analysis types are classification and regression. 2. Unsupervised machine learning: used when you do not know your training data e.g. when grouping or categorizing costumers based on their shopping behaviour.   common analysis types are clustering and dimensionality reduction. In semi-supervised machine learning the data includes some labels, but not for everything. 3. Reinforcement machine learning  uses training data with a feedback loop  e.g. a rewarding system. It is used to study which rewards makes a costumer a long-term loyal costumer. rather expensive and only used to study long-term goals.    Deep learning is used for the analysis of multiple layers   e.g. image processing, for which one layer looks at the edges, another layer looks at colors and yet another layer tries to identify concepts that are relevant to, for example, humans.   www.kaggle.com is a website with many freely available datasets that can be analyzed using Python and R. It provides code for tweaking visualizations.  "
34,day34-20201112.md,Tammy Sexton,1605199546,"Day 34 - Thursday, 12th Novmeber 2020 Clustering Continued Protocol by Laura  Schedule  9:00 - 9:30 Daily review 9:30 - 10:00 Coffee break 10:00 - 10:10 Check in with Tereza and Mia 10:10 - 12:00 Notebook 12:00 - 13:00 Lunch 13:00 - 13:20 Check in with Tereza and Mia 13:20 onwards Notebook  Link to notebooks https://github.com/neuefische/2020-ds-Dimensionality-Reduction Link to spreadsheet on capstone ideas https://docs.google.com/spreadsheets/d/1Z5hxFTsh3TZIjktIlOSdCiwMymUKtQO77p7rr_6yfKE/edit?usp=sharing Key things from notebooks Affinity Propagation Clustering  Creates clusters by sending messages between pairs of samples until convergence Dataset is then described using small number of exemplars, which are identified as those most representative of other samples Messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs This updating happens iteratively until convergence, at which point the final exemplars are chosen and hence the final clustering is given Automatically chooses the number of clusters based on the data provided Main drawback of Affinity Propagation is its complexity Link from Mia: https://towardsdatascience.com/unsupervised-machine-learning-affinity-propagation-algorithm-explained-d1fef85f22c8  Gaussian Mixture Model (GMM)  Attempts to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments—in Scikit-Learn this is done using the predict_proba method, which returns a matrix of size [n_samples, n_clusters] which measures the probability that any point belongs to the given cluster Can be visualized by making the size of each point proportional to the certainty of its prediction Uses an expectation-mazimization approach Choose starting guesses for the location and shape Repeat until converged: E-step (Expectation): for each point, find weights encoding the probability of membership in each cluster M-step (Maximization): for each cluster, update its location, normalization, and shape based on all data points, making use of the weights   Results in each cluster being associated not with a hard-edged sphere but with a smooth Gaussian model Choosing the covariance type: covariance_type=""diag"" (default), means that the size of the cluster along each dimension can be set independently, with the resulting ellipse constrained to align with the axes covariance_type=""spherical"", constrains the shape of the cluster such that all dimensions are equal; result will have similar characteristics to that of k-means, though it is not entirely equivalent covariance_type=""full"", allows each cluster to be modeled as an ellipse with arbitrary orientation Akaike information criterion (AIC): estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model; deals with the trade-off between the goodness of fit of the model and the simplicity of the model Bayesian information criterion (BIC): based, in part, on the likelihood function, and it is closely related to AIC; only difference is that BIC considers the number of observations in the formaula, which AIC does not "
35,day13_20201014.md,Kristina Norris,1602703227,"Day 13 - Wednesday, 14th October 2020 Going on with the EDA project Protocol by Sebastian Schedule   9.00 - 9.30: Daily review of day 12   09.30 - 10.00:  Q&A about our progress on EDA and problems that occured    How & when did you deal with missing values and how did you detect them?  After splitting the set  No general answer but some approaches at:  https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b   how did you partially transform data within a colum to create a dummy?  with .apply or .eval example: king_county['sqft_basement'].apply(lambda x: 1 if x > 0 else 0)    How did you prepare for the stakeholders Q&A?     10.00 - 10.30:  Recap of our progress with Mia & Tereza    Slow progress for most of us so far, uncertainty about where to go  Pointing out we should have 2 notebooks, one workbook and one for presentation + our training script as .py file in our repository + readme file Presentation notebook should answer our stakeholders question and be presented within 10 minutes  Repository should be uploaded by thursday 12am    10.30 - 11:45 Stakeholder Q&A    We get to know our stakeholders:   Who | Stakeholder | Characteristics -----|-------|------ Ole | Buyer | 5 kids, no money, nice (social) neighbourhood, Timing?, Location? Petra | Seller | Invest with big returns, renovation?, which Neighbourhood? Timing? Andrew | Seller | Has house moves soon (timing?), high profit in middle class NH Jakolien | Buyer | Waterfront , limited budget, nice & isolated but central neighbourhood without kids (but got some on her own) Michael | Buyer | Lively, central neighbourhood, middle price range, right timing (within a year) Sina | Buyer | High budget, wants to show off, timing within a month, waterfront, renovated, high grades, resell within 1 year Martina | Seller | Has several houses, some in bad neighbourhoods, willing to evict people, timing?, big returns, open for renovations Kevin | Buyer | 2 people, country (best timing & unrenovated) & city house (fast & central location) Andreas | Buyer | Invest in poor neighbourhood, buying & selling, costs back + little profit, socially responsible Sebastian | Buyer | Unlimited Budget, 4+ bathrooms or smaller house nearby, big lot (tennis court & pool), golf, historic, no waterfront Jennifer | Seller | Invests in historical houses, best NHs, high profits, best timing within a year Chandra | Seller | Owns expensive houses, needs to get rid, best timing within a year, open for renovation when profits rise Laura | Seller | Italian mafiosi, sells several central houses(top10%) over time, needs average outskirt houses over time   11.45 - 15.00: Work on EDA project   Including lunch break   15.00 - 15.30: Check up on progress    still slow progress ->long evening  Tereza's advice: ""Save time by sharing codes & don't get hung up on details.""  really helpfulllink for visualizing data on maps in python:  https://www.bigendiandata.com/2017-06-27-Mapping_in_Jupyter/    15.30 - 23.59: Work on EDA project  "
36,day39_20201119.md,Ellen Henderson,1605807146,"Day 39 - Thursday, 19 November 2020 Sequence Models Protocol by Ole    Schedule   09:00 - 10:00: Daily review of day 38 by Petra   10:00 - 10:30: Check-in with Tereza and Mia   10:30 - 12:00: Lecture: Sequence Models    12:00 - 13:00: Lunch break   13:00 - 17:00: Notebooks on Sequence Models   Check-in with Tereza and Mia Tips for Applications: * Trade CV's with friends * Measure your own improvements/sucesses Questions to Tereza & Mia resulting from the morning review: * Q: How do you use regularization with many layers? Apply it to every layer separately or once at start? * A: If you use regularization, apply it to every layer that uses weights (not on a Pooling layer for example) - However, the regularization parameter (degree of regularization determined by lambda) can be different for the layers. Droput percentages can also be of different values for different layers. (Educational) Repo for applying a neural network to the fashion data set: https://github.com/mjbhobe/dl-tensorflow-keras/blob/master/Fashion%20-%20CNN%20-%20Keras.ipynb Explanation of how to regularize your neural network to improve performance: https://towardsdatascience.com/how-to-improve-a-neural-network-with-regularization-8a18ecda9fe3 Lecture: Sequence Models For details, see slides ""18 - Sequence Models"" Sequence data is data that can be meaningfully ordered  Sequence data mostly has and can work with variable inputs, unlike CNN's that require for example images of the same size  Recurrent Neural Networks  A Recurrent Neural Network (RNN) remembers what it learned in the past It updates its inner state permanently Text data needs to be prepared for the model (One-Hot-Encoder) Limitations: Problems with long-term sequences Vanishing/Exploding gradients are an even more serious problem here L1/L2 regularization can help Weight initialization Gradient clipping ReLu activation function   Does not learn temporal correlations  How RNN's work - matrix simulation: https://www.youtube.com/watch?v=ppz0XdEcGF4 Post on RNN on Andrej Karpathy' Blog: http://karpathy.github.io/2015/05/21/rnn-effectiveness/ LSTM (Long-Short-Term-Memory) How do the L and S get into the LSTM?  h is the hidden state and carries the Short-term info c is the cell state and carries the Long-term info  Gates to the resue.  Forget gate: learns to erase memories that are not need Update gate: learns what information should be stored in the cell state (long-term memory) Output gate: defines which info from the cell state should become part of the hidden state  GRU (Gated Recurrent Units) What is the difference to LSTM?  GRU only uses a single state (c) Forgetting and updating is done in one operation (gate) Reset gate: resets the cell state based on how much old cell state information gets used when forming a new candidate Does not have an output gate - output is defined by the cell state  Applications  Sequence-to-One Sentiment analysis Forecasting Smartreply One-to-Sequence Image captioning Music generation Sequence-to-Sequence Machine translation Speech-to-text  All things LSTM and GRU: https://www.youtube.com/watch?v=8HyCNIVRbSU Notebooks - Sequence Models Notebook 1: GRU How to predict nationalities from names. Notebook 2: LSTM How to predict income (from nationalities) from names."
37,day11_20201012.md,Ellen Henderson,1602531981,"Day 11 - Monday, 12 October 2020 Regression Analysis Protocol by Ole Schedule  9- 10: Daily review of day 10  Some insights on git from Chandra:   * git remote -v shows current origin and upstream of the repository on the machine   * git remote add/remove upstream/origin <url> adds or removes an upstream or origin   * git pull upstream master updates repository on the machine by pulling files from upstream master branch   * git push origin master updates your fork in the git-hub online repository Addition from Tereza:   * It is more common to use pull requests when working with others (in a company) to have more control over the flow of information  10 - 11: Presentations on visualization Matplotlib Seaborn Bokeh  Plotly   11-12: Lecture: Linear Regression    12 - 13: Lunch break   13 - 16: Group work: 6 out of 7 Notebooks on linear regression   16 - 16:30: Daily check out   Presentations on visualization Full presentations can be found at -> https://github.com/neuefische/hh-2020-ds4-daily-review/tree/master/Visualization-presentations Part I: Matplotlib by Petra, Chandra, Jakolien, Ole   Functionalities: Plot simple graphs   Set-up: A simple import line is sufficient   Pro's and con's:    Good for basic plots (+) Quick and easy data visualizations (+) Good for basic plots, but not for extensive visualizations (-) Few possibilities to adjust the plots to look nice (-)    Part II: Seaborn by Martina, Sina, Jenny   Functionalities: Many different plots, color palettes   Set-up: Requires Python 3.6+ - in addition, numpy, pandas, scipy and matplotlib are needed   Pro's and con's:  Accepts many data structures (+) Easily build complex visualizations (+) Dependency on other libraries (-)    Part III: Bokeh by Andreas, Andrew, Kevin, Sebastian   Functionalities: Uses glyphs (visual shapes), can work with streamed data from f.e. REST calls   Set-up: A simple import is enough, but the whole package is rather large   Pro's and con's:  Interactive visualization of large and complex data sets (+) Open source (+) No inherent 3D graphing (-) Takes more code than seaborn or plotly (-)    Part IV: Plotly by Laura, Volodymyr, Michael   Functionalities: Interactive plots, can be used for dashboarding   Set-up: More complicated, see presentation for instructions   Pro's and con's:  Easy to use (+) Beautfiul visualizations (+) Setup is challenging (-)    Lecture: Linear regression For details, see slides ""6_Linear_Regression"" Important takeaways:   * Understanding linear regression is important before one moves on and applies more complex approaches like neural networks   * Regression cannot prove causality   * $R^2$ of a model should be evaluated when the goal is explanation (descriptive statistics)   * $RMSE$ of a model should be evaluated when the goal is prediction (predictive statistics) Notebooks Linear regression technqiues Assumption: Data has been cleaned and prepared and is available in the vector y and vector/matrix X. Approach 1 import statsmodels.api as sms Command | Explanation -----|------- X = sms.add_constant(X) | Allows for an intercept when using X as explanatory variable(s) model = sms.OLS(y, X) | Create an OLS (Ordinary Least Squares) model using X and the target variable y results = model.fit() | Fits the OLS model to the data given by X and y results.summary() | Prints an overview of the regression results Approach 2 import statsmodels.formula.api as smf Command | Explanation -----|------- model = smf.ols(formula='y ~ X1 + X2 + X3', data=data) | Create an OLS (Ordinary Least Squares) model using X1, X2 and X3 to describe the target variable y results = model.fit() | Fits the OLS model to the data given by X and y results.summary() | Prints an overview of the regression results Note:    * Approach 1 needs to have a constant inserted into the data to allow for intercept, uses sms.OLS and (y, [X]) as input, Approach 2 uses sm.ols and (formula='y ~ X1 + X2 + X3 + ... XN', data=data) as input   * results.rsquared and results.rsquared_adjcan be used to extract R^2 and its adjusted value directly from the summary - will work for any of the entries in the summary Notebook 1: Linear regression Notebook 2: Limitations of linear regression Notebook 3: Linear regression exercise Notebook 4: Multiple Regression Notebook 5: Multiple Regression Exercise Notebook 6: Solutiuon to Notebook 5"
38,day36_20201116 (NN).md,Sara Dawson,1605570180,"Day 36 - Monday, 16th November 2020 Protocol by Michael Artificial Neural Networks (ANN)   Schedule  09:00 - 09:30: Daily review - Unsupervised Learning (Clustering) 10:00 - 12:30: Lecture - Artificial Neural Networks 12:30 - 13:30: Lunch break 13:30 - 13:45: Working on notebooks 1-3 16:00 - 16:30: Daily check out 16:30 - 17:30: Continuation - Working on notebooks 1-3   THE END   Preparation on ANN  Stanford  Lecture 4 | Introduction to Neural Networks   https://www.youtube.com/watch?v=d14TUNcbn1k&list=PLKPJtUGFjr1v8JvbezsTl36GT3B9TvprL&index=1 Lecture 5 | Convolutional Neural Networks   https://www.youtube.com/watch?v=bNb2fEVKeEo&list=PLKPJtUGFjr1v8JvbezsTl36GT3B9TvprL&index=3 CS231n Winter 2016: Lecture 10: Recurrent Neural Networks, Image Captioning, LSTM https://www.youtube.com/watch?v=yCC09vCHzF8 Notes   https://cs231n.github.io Slides for Lecture 4:   http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf  StatQuest  Neural Networks Part 1: Inside the Black Box   https://www.youtube.com/watch?v=CqOfi41LfDw&list=PLKPJtUGFjr1v8JvbezsTl36GT3B9TvprL&index=3  Neural Networks Part 2: Backpropagation Main Ideas    https://www.youtube.com/watch?v=IN2XmBhILt4&list=PLKPJtUGFjr1v8JvbezsTl36GT3B9TvprL&index=4   Artificial Neural Networks (Summary) Quote Tereza: ""btw.. the neural network is overwhelming at first.. but then you will all find out that it's all yet another bunch of models just with more options..""  What is a Artificial-NN?  (A)NNs are a class of machine learning algorithms (also classified within deep learning - as a subfield of machine learning) NNs are computing systems inspired by the biological neural networks (e.g. the human brain) A NN is a web of interconnected entities known as nodes wherein each node is responsible for a simple computation (a decision) Simple as that - ANNs are algorithms   When and why do we use ANNs?  Could be used all the time respectively applied to all kinds of questions and provide accurate answers to some complex problems, such as natural language processing, image recognition,  forecasting, risk manage [...]   How does a ANN work?  The NN takes an input The input gets passed through multiple layers of hidden neurons (mini-functions with unique coefficients that must be learned) The output is a prediction representing the combined input of all the neurons    The reproducibility is given by memorizing the trained weights in a matrix  [...] In other words [...] https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9 [...] and in different words [...] https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#weights  What does Backpropagation do?  The goal of backpropagation is to adjust each weight in the network in proportion to how much it contributes to overall error. If we iteratively reduce each weight’s error, eventually we’ll have a series of weights that produce good predictions. Pattern in backpropagation: Add gate Max gate Mul gate     Gradients are the effects of changing something Chain rule:    The Chain rule helps to identify how much each weight contributes to our overall error and the direction to update each weight to reduce our error. An example:      Stanford Example - Computational Tree       And in general:  [s] - https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html  [s] - http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf  Possible activation functions  Sigmoid Tanh ReLU (Rectified Linear Unit) [...] https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/    What Hyperparameters do exist? | Command | Description | | ------- | ----------- | | Learning rate | very important | | Number of layers | not so much | | Number of neurons per layer | important | | Number of features | important | | Mini-batch size | important | | Optimization algorithm | important | | Learning rate decay | not so much |  Early stopping  Early stopping is used to find the optimal point (validation error raises again) Builds on the development of weights during training (weights start small and increase) Not the best method to regularize (orthogonalization of optimizing and regularizing gets lost)   What is the difference between a NN and a Deep-NN?  A NN can be shallow - it has an input layer of neurons, a hidden layer (processing the inputs) and an output layer (final output of the model) A Deep-NN commonly has between 2-8 additional layers of neurons Accuracy increases with the number of hidden layers    Epochs  [...]   Commonly used Deep Learning Framework  TensorFlow - widely used FW python Keras - high level Application Programming Interface (API)from TensorFlow PyTorch [...] further possible Framework see slides    Tipps & Tricks  Google Colab - Similar to Jupyter Lab but with all libraries installed (big Virtual Environment) Pre-Trained models can be used to reduce the training effort (pre-trained models have been trained on large data sets. By applying them, the weights and architecture can be applied directly on a problem statement) Navigation between GitHub-Branches  | Command | Description |   | ------- | ----------- |   | git branch | List branches (the asterisk denotes the current branch) |   | git branch -a | List all branches (local and remote) |   | git branch [branch name] | Create a new branch |   | git branch -d [branch name] | Delete a branch |   | git push origin --delete [branch name] | Delete a remote branch |   | git checkout -b [branch name] | Create a new branch and switch to it |   | git checkout -b [branch name] origin/[branch name] | Clone a remote branch and switch to it |   | git branch -m [old branch name] [new branch name] | Rename a local branch |   | git checkout [branch name] | Switch to a branch |   | git checkout - | Switch to the branch last checked out |   | git checkout -- [file-name.txt] | Discard changes to a file |   | git merge [branch name] | Merge a branch into the active branch |   | git merge [source branch] [target branch] | Merge a branch into a target branch | [s] - https://github.com/joshnh/Git-Commands  Additional Links [s] - Tensorflow Playground - https://playground.tensorflow.org/  Sources [s] - https://en.wikipedia.org/wiki/Artificial_neural_network  [s] - https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html  [s] - https://www.upgrad.com/blog/machine-learning-vs-neural-networks/  [s] - https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/  [s] - https://github.com/FischJunge/Git-Commands  [s] - https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9    Repo - 2020-ds-Intro-to-Tensorflow Link: https://github.com/neuefische/2020-ds-intro-to-tensorflow !!! NOTEBOOKS ARE LOCATED IN DIFFERENT BRANCHES !!! Notebook 1: Colab Link: https://github.com/neuefische/2020-ds-intro-to-tensorflow  File: 0-Colab.ipynb Notebook 2: Basic Regression (Predict fuel efficiency) Link: https://github.com/neuefische/2020-ds-intro-to-tensorflow/tree/first-iteration  File: 0.1-Regression_with_TensorFlow_Keras.ipynb Notebook 3:  Logistic Regression Tensorflow Link: https://github.com/neuefische/2020-ds-intro-to-tensorflow/tree/first-iteration  File: 0.2-Classification_with_TensorFlow.ipynb"
